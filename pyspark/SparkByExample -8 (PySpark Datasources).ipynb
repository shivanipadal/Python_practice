{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "982e5c2a-c476-425f-9c60-cfe77418dac7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##PySpark Datasources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d4fc11-64c3-49db-9618-c608d77a79cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###PySpark Read CSV file into DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ed51b9-6043-474a-a990-1f572f702c2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbaabae0-205f-43b6-8fbf-556fa3c5beea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark=SparkSession.builder \\\n",
    "      .master('local[*]') \\\n",
    "      .appName('SparkByExamples') \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9179cc-e51a-4b9a-b50c-2f6b34941eca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- RecordNumber: integer (nullable = true)\n |-- Zipcode: integer (nullable = true)\n |-- ZipCodeType: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- LocationType: string (nullable = true)\n |-- Lat: double (nullable = true)\n |-- Long: double (nullable = true)\n |-- Xaxis: double (nullable = true)\n |-- Yaxis: double (nullable = true)\n |-- Zaxis: double (nullable = true)\n |-- WorldRegion: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- LocationText: string (nullable = true)\n |-- Location: string (nullable = true)\n |-- Decommisioned: boolean (nullable = true)\n |-- TaxReturnsFiled: integer (nullable = true)\n |-- EstimatedPopulation: integer (nullable = true)\n |-- TotalWages: integer (nullable = true)\n |-- Notes: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/spark-examples/pyspark-examples/blob/master/resources/zipcodes.csv \n",
    "\n",
    "Filepath : \"File uploaded to /FileStore/tables/zipcodes.csv\"\n",
    "df = spark.read.options(delimiter=\",\").csv(\"/FileStore/tables/zipcodes.csv\", header=True, inferSchema=True)\n",
    "# or \n",
    "\n",
    "df = spark.read.options(header='True', inferSchema='True', delimiter=',').csv('/FileStore/tables/zipcodes.csv')\n",
    "df.printSchema()\n",
    "\n",
    "# Read Multiple CSV Files\n",
    "# df = spark.read.csv(\"path1,path2,path3\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66aa9425-3aaf-485a-9485-95aa2a57ace4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reading CSV files with a user-specified custom schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27a122f2-2362-4f28-b706-f75304de201a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- RecordNumber: integer (nullable = true)\n |-- Zipcode: integer (nullable = true)\n |-- ZipCodeType: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- LocationType: string (nullable = true)\n |-- Lat: double (nullable = true)\n |-- Long: double (nullable = true)\n |-- Xaxis: integer (nullable = true)\n |-- Yaxis: double (nullable = true)\n |-- Zaxis: double (nullable = true)\n |-- WorldRegion: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- LocationText: string (nullable = true)\n |-- Location: string (nullable = true)\n |-- Decommisioned: boolean (nullable = true)\n |-- TaxReturnsFiled: string (nullable = true)\n |-- EstimatedPopulation: integer (nullable = true)\n |-- TotalWages: integer (nullable = true)\n |-- Notes: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType, BooleanType\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"RecordNumber\",IntegerType(),True) \\\n",
    "      .add(\"Zipcode\",IntegerType(),True) \\\n",
    "      .add(\"ZipCodeType\",StringType(),True) \\\n",
    "      .add(\"City\",StringType(),True) \\\n",
    "      .add(\"State\",StringType(),True) \\\n",
    "      .add(\"LocationType\",StringType(),True) \\\n",
    "      .add(\"Lat\",DoubleType(),True) \\\n",
    "      .add(\"Long\",DoubleType(),True) \\\n",
    "      .add(\"Xaxis\",IntegerType(),True) \\\n",
    "      .add(\"Yaxis\",DoubleType(),True) \\\n",
    "      .add(\"Zaxis\",DoubleType(),True) \\\n",
    "      .add(\"WorldRegion\",StringType(),True) \\\n",
    "      .add(\"Country\",StringType(),True) \\\n",
    "      .add(\"LocationText\",StringType(),True) \\\n",
    "      .add(\"Location\",StringType(),True) \\\n",
    "      .add(\"Decommisioned\",BooleanType(),True) \\\n",
    "      .add(\"TaxReturnsFiled\",StringType(),True) \\\n",
    "      .add(\"EstimatedPopulation\",IntegerType(),True) \\\n",
    "      .add(\"TotalWages\",IntegerType(),True) \\\n",
    "      .add(\"Notes\",StringType(),True)\n",
    "      \n",
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/FileStore/tables/zipcodes.csv\")\n",
    "\n",
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7a51d15-f521-4873-b8b7-5429347e5cf7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Saving mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a29dc23-16d5-492f-9d04-c44cdb95ee31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_schema.write.mode('overwrite').csv('FileStore/tables/modified_zipcoe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4c0e2db-67c9-4ce1-8fce-619746987f98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark Read and Write Parquet File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8884ab5b-bf5e-40e5-99e0-a711951cb85d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* **What is Parquet File?**\n",
    "\n",
    "Apache parquet file is a column storage format available to any project in the hadoop ecosystem\n",
    "\n",
    "- Advantages: \n",
    "  \n",
    "  - While querying columnar storage, it skips the non-relevant data very quickly, making fatser query execution. As a result aggrgation queries consule less time compared to row-oriented databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a96b8f6-1987-40af-affe-16fa99307a81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "data = [(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.mode('overwrite').parquet('/FileStore/tables/people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deffc570-a195-465c-95a7-2f84bd27e5f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|  dob|gender|salary|\n+---------+----------+--------+-----+------+------+\n|  Robert |          |Williams|42114|     M|  4000|\n|   Maria |      Anne|   Jones|39192|     F|  4000|\n| Michael |      Rose|        |40288|     M|  4000|\n|   James |          |   Smith|36636|     M|  3000|\n|      Jen|      Mary|   Brown|     |     F|    -1|\n+---------+----------+--------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "parDF1= spark.read.parquet('/FileStore/tables/people.parquet')\n",
    "parDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b929f792-fc66-4895-9ced-96d01d898513",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|  dob|gender|salary|\n+---------+----------+--------+-----+------+------+\n|  Robert |          |Williams|42114|     M|  4000|\n|   Maria |      Anne|   Jones|39192|     F|  4000|\n| Michael |      Rose|        |40288|     M|  4000|\n|   James |          |   Smith|36636|     M|  3000|\n|      Jen|      Mary|   Brown|     |     F|    -1|\n+---------+----------+--------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "parDF1.createOrReplaceTempView('parquetTable')\n",
    "spark.sql('select * from parquetTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a552d5bc-9da0-4ec6-abe7-d8d6ec4b9260",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.partitionBy('gender','salary').mode('overwrite').parquet('/FileStore/tables/people_partitions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb7efdd-1dd3-4efd-bcff-d1f1b9a80f1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+\n|firstname|middlename|lastname|  dob|\n+---------+----------+--------+-----+\n|  Robert |          |Williams|42114|\n| Michael |      Rose|        |40288|\n+---------+----------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "parDF2=spark.read.parquet('/FileStore/tables/people_partitions.parquet/gender=M/salary=4000')\n",
    "parDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf55e28-d366-4dba-9247-0938401f7714",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql('create temporary view person2 using parquet options(path \\\"/FileStore/tables/people_partitions.parquet/gender=F\\\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93856b16-5d95-411d-96b2-172b897bc20b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+\n|firstname|middlename|lastname|  dob|salary|\n+---------+----------+--------+-----+------+\n|   Maria |      Anne|   Jones|39192|  4000|\n|      Jen|      Mary|   Brown|     |    -1|\n+---------+----------+--------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from person2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef59130a-42a4-46d0-890a-66f6067f6cc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###PySpark Read JSON file into DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef4d567-3887-43bb-a5c3-7464163df024",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df= spark.read.json(\"resource/zipcodes.json\")\n",
    "\n",
    "#read multiline json file\n",
    "\n",
    "# multiline_df = spark.read.option('multiline', 'true').json('resources/multiline-zipcode.json')\n",
    "# multiline_df.show()\n",
    "\n",
    "#read multiple files\n",
    "# df2=spark.read.json(['resources/zipcode2.json', 'resources/zipcode1.json'])\n",
    "# df2.show()\n",
    "\n",
    "#Read All JSON files from a directory\n",
    "# df3=spark.read.json('resources/*.json')\n",
    "# df3.show()\n",
    "\n",
    "# Create a table from json File\n",
    "# spark.sql(\"create or replace temporary table zipcode3 using json options\" + \"(path 'resources/zipcodes.json')\")\n",
    "# spark.sql('select * from zipcode3').show()\n",
    "\n",
    "\n",
    "# PySpark write Parquet File\n",
    "# df2.write.mode('Overwrite').json('/tmp/spark_output/zipcodes.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a844d678-21c7-4545-850a-8045ad48daa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkByExample -8 (PySpark Datasources)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
