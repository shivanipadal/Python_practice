{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa515d56-6f02-44da-9f8b-d635b8d380e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark Random Sample with Example\n",
    "It is a mechanism to get random sample records from dataset, this is helpful when you have a larger dataset and wanted to analyze/test a subset of the data example 10% of the original file\n",
    "\n",
    "for Datafrmae:\n",
    "Synatx: `sample(withReplacement=False, fraction, seed=None)`\n",
    "\n",
    "for RDD:\n",
    "syntax:`sample(self, withReplacement, fraction, seed=None) `\n",
    "\n",
    "\n",
    "`fraction`:  Fraction of rows to generate, range [0.0, 1.0]. Note that it doesn’t guarantee to provide the exact number of the fraction of records.\n",
    "\n",
    "`seed`: Used to reproduce the same random sampling\n",
    "\n",
    "`withReplacement`: Sample with replacement or not. Bydefault it's False.If the value is True then the some of the values are duplicated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8a72ee3-1883-4642-a6ee-67ec599e4385",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Using fraction to get a random sample in PySpark\n",
    "\n",
    "By using fraction between 0 to 1, it returns the approximate number of the fraction of the dataset. For example, 0.1 returns 10% of the rows. However, this does not guarantee it returns the exact 10% of the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5772a37f-cfab-4049-8ac5-b58865d268b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('SparkByExample.com') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51878c8-ccd2-4006-9e4e-ce93e2a71bb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=22), Row(id=28), Row(id=38), Row(id=55), Row(id=82), Row(id=92)]\n"
     ]
    }
   ],
   "source": [
    "df=spark.range(100)\n",
    "print(df.sample(0.06).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a4b1df-df82-4be8-9b3d-00600c1e4300",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 48, 53, 60, 72, 87, 91, 96, 98]\n[0, 11, 16, 18, 19, 23, 23, 24, 26, 26, 27, 29, 35, 38, 47, 49, 54, 54, 55, 61, 61, 66, 68, 81, 81, 82, 85, 97, 99]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rdd = spark.sparkContext.range(0,100)\n",
    "print(rdd.sample(False,0.1,0).collect())\n",
    "print(rdd.sample(True,0.3,123).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c825130a-b394-49e5-accc-aed003f53bbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####1.2 Using seed to reproduce the same Samples in PySpark\n",
    "\n",
    "To get consistent same random sampling uses the same slice value for every run. Change slice value to get different results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c128e960-72cb-4f65-8760-f2d6956667c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=35), Row(id=38), Row(id=41), Row(id=45), Row(id=71), Row(id=84), Row(id=87), Row(id=99)]\n[Row(id=35), Row(id=38), Row(id=41), Row(id=45), Row(id=71), Row(id=84), Row(id=87), Row(id=99)]\n[Row(id=22), Row(id=33), Row(id=35), Row(id=41), Row(id=53), Row(id=80), Row(id=83), Row(id=87), Row(id=92)]\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(0.1,123).collect())\n",
    "print(df.sample(0.1,123).collect())\n",
    "print(df.sample(0.1,456).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30a0fd9a-4e52-4ad3-8efe-4864a1dfe3fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3 Sample withReplacement (May contain duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf2b114-d26c-4f0f-b7e7-8196a4ba99da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [Row(id=0),\n Row(id=4),\n Row(id=12),\n Row(id=15),\n Row(id=19),\n Row(id=21),\n Row(id=23),\n Row(id=24),\n Row(id=25),\n Row(id=28),\n Row(id=29),\n Row(id=34),\n Row(id=35),\n Row(id=36),\n Row(id=38),\n Row(id=41),\n Row(id=45),\n Row(id=47),\n Row(id=50),\n Row(id=52),\n Row(id=59),\n Row(id=63),\n Row(id=65),\n Row(id=71),\n Row(id=82),\n Row(id=84),\n Row(id=87),\n Row(id=94),\n Row(id=99)]"
     ]
    }
   ],
   "source": [
    "df.sample(True, 0.3, 123).collect() #with Duplicates\n",
    "df.sample(0.3, 123).collect() #No duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bba3f9b2-72d0-45a1-b1a7-ef2daea69035",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark fillna() & fill() – Replace NULL/None Values\n",
    "\n",
    "syntax: \n",
    "\n",
    "`fillna(value, subset=None)`\n",
    "\n",
    "`fill(value, subset=None)`\n",
    "\n",
    "**value** – Value should be the data type of int, long, float, string, or dict. Value specified here will be replaced for NULL/None values.\n",
    "\n",
    "**subset** – This is optional, when used it should be the subset of the column names where you wanted to replace NULL/None values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb774484-622b-4917-8ee9-d5dc52021943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- zipcode: integer (nullable = true)\n |-- type: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- population: integer (nullable = true)\n\n+---+-------+--------+-------------------+-----+----------+\n|id |zipcode|type    |city               |state|population|\n+---+-------+--------+-------------------+-----+----------+\n|1  |704    |STANDARD|null               |PR   |30100     |\n|2  |704    |null    |PASEO COSTA DEL SUR|PR   |null      |\n|3  |709    |null    |BDA SAN LUIS       |PR   |3700      |\n|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n|5  |76177  |STANDARD|null               |TX   |null      |\n+---+-------+--------+-------------------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#The file we are using here is available at GitHub small_zipcode.csv\n",
    "# https://github.com/spark-examples/spark-scala-examples/blob/master/src/main/resources/small_zipcode.csv\n",
    "\n",
    "\n",
    "filepath='/FileStore/tables/small_zipcode.csv'\n",
    "df=spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea4625f-c97f-44dc-8082-037d068fe900",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n| id|zipcode|    type|               city|state|population|\n+---+-------+--------+-------------------+-----+----------+\n|  1|    704|STANDARD|               null|   PR|     30100|\n|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n|  5|  76177|STANDARD|               null|   TX|         0|\n+---+-------+--------+-------------------+-----+----------+\n\nOut[12]: DataFrame[id: int, zipcode: int, type: string, city: string, state: string, population: int]"
     ]
    }
   ],
   "source": [
    "\n",
    "#Replace 0 for null for all integer columns\n",
    "df.na.fill(value=0).show()\n",
    "#Replace 0 for null on only population column \n",
    "df.na.fill(value=0, subset=['population'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54e32597-9e9a-4b8f-849c-7e8b67c71f4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### PySpark Replace Null/None Value with Empty String\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02bdf43-2dde-4766-89e4-3fbeb850735f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n| id|zipcode|    type|               city|state|population|\n+---+-------+--------+-------------------+-----+----------+\n|  1|    704|STANDARD|                   |   PR|     30100|\n|  2|    704|        |PASEO COSTA DEL SUR|   PR|      null|\n|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n|  5|  76177|STANDARD|                   |   TX|      null|\n+---+-------+--------+-------------------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(value='').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8abefc5b-370a-4536-83d4-f36c78968bba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n| id|zipcode|    type|               city|state|population|\n+---+-------+--------+-------------------+-----+----------+\n|  1|    704|STANDARD|            unknown|   PR|     30100|\n|  2|    704|        |PASEO COSTA DEL SUR|   PR|         0|\n|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n|  5|  76177|STANDARD|            unknown|   TX|         0|\n+---+-------+--------+-------------------+-----+----------+\n\n+---+-------+--------+-------------------+-----+----------+\n| id|zipcode|    type|               city|state|population|\n+---+-------+--------+-------------------+-----+----------+\n|  1|    704|STANDARD|            unknown|   PR|     30100|\n|  2|    704|        |PASEO COSTA DEL SUR|   PR|         0|\n|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n|  5|  76177|STANDARD|            unknown|   TX|         0|\n+---+-------+--------+-------------------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Now, let’s replace NULL’s on specific columns, below example replace column type with empty string and column city with value “unknown”.\n",
    "\n",
    "df.na.fill('unknown',['city']) \\\n",
    "    .na.fill(\"\", ['type']) \\\n",
    "    .na.fill(0,['population']).show() \n",
    "\n",
    "#Alternatively you can also write the above statement as\n",
    "\n",
    "df.na.fill({'city':'unknown', 'type':\"\", 'population':0}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722ffd29-3248-4547-bb1c-8496a472301b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Product: string (nullable = true)\n |-- Amount: long (nullable = true)\n |-- Country: string (nullable = true)\n\n+-------+------+-------+\n|Product|Amount|Country|\n+-------+------+-------+\n|Banana |1000  |USA    |\n|Carrots|1500  |USA    |\n|Beans  |1600  |USA    |\n|Orange |2000  |USA    |\n|Orange |2000  |USA    |\n|Banana |400   |China  |\n|Carrots|1200  |China  |\n|Beans  |1500  |China  |\n|Orange |4000  |China  |\n|Banana |2000  |Canada |\n|Carrots|2000  |Canada |\n|Beans  |2000  |Mexico |\n+-------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ee1c5b-1f00-4037-85c4-f50d6bde8e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n|Product|Canada|China|Mexico| USA|\n+-------+------+-----+------+----+\n| Orange|  null| 4000|  null|4000|\n|  Beans|  null| 1500|  2000|1600|\n| Banana|  2000|  400|  null|1000|\n|Carrots|  2000| 1200|  null|1500|\n+-------+------+-----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Product').pivot('Country').sum('Amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4b4a151-4570-4754-ad12-8907e9c7b111",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+-------+------+\n|Country|Banana|Beans|Carrots|Orange|\n+-------+------+-----+-------+------+\n|  China|   400| 1500|   1200|  4000|\n|    USA|  1000| 1600|   1500|  4000|\n| Mexico|  null| 2000|   null|  null|\n| Canada|  2000| null|   2000|  null|\n+-------+------+-----+-------+------+\n\n+-------+----+-----+------+------+\n|Product| USA|China|Canada|Mexico|\n+-------+----+-----+------+------+\n| Orange|4000| 4000|  null|  null|\n|  Beans|1600| 1500|  null|  2000|\n| Banana|1000|  400|  2000|  null|\n|Carrots|1500| 1200|  2000|  null|\n+-------+----+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Country').pivot('Product').sum('Amount').show()\n",
    "\n",
    "#  pivot is a very expensive operation hence, it is recommended to provide column data (if known) as an argument to function as shown below.\n",
    "countries = [\"USA\",\"China\",\"Canada\",\"Mexico\"]\n",
    "df.groupBy('Product').pivot('Country', countries).sum('Amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e70b875-5d13-40e1-af1a-0578e3fbd836",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n|Product|Canada|China|Mexico|USA |\n+-------+------+-----+------+----+\n|Orange |null  |4000 |null  |4000|\n|Beans  |null  |1500 |2000  |1600|\n|Banana |2000  |400  |null  |1000|\n|Carrots|2000  |1200 |null  |1500|\n+-------+------+-----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pivotDF = df.groupBy(\"Product\",\"Country\") \\\n",
    "      .sum(\"Amount\") \\\n",
    "      .groupBy(\"Product\") \\\n",
    "      .pivot(\"Country\") \\\n",
    "      .sum(\"sum(Amount)\")\n",
    "pivotDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95e9f2b7-b26f-43aa-b3c1-a683668359d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Unpivot PySpark DataFrame\n",
    "\n",
    "Pyspark sql doesnot have unpivot function hence will use the **stack()** function....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5e9513-4446-4851-a202-140d58f9c318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n|Product|Country|Total|\n+-------+-------+-----+\n| Orange|  China| 4000|\n|  Beans|  China| 1500|\n|  Beans| Mexico| 2000|\n| Banana| Canada| 2000|\n| Banana|  China|  400|\n|Carrots| Canada| 2000|\n|Carrots|  China| 1200|\n+-------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "unpivotexpr = \"stack(3, 'Canada', Canada,'China', China,'Mexico', Mexico) as (Country, Total)\"\n",
    "unpivotDF = pivotDF.select('Product', expr(unpivotexpr)).where('Total is not null')\n",
    "unpivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6089d56-094d-4afc-bbed-092a578eb75f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark partitionBy() – Write to Disk Example\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-partitionby-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60e57bdb-4bbb-4821-a70a-19b8edee6dab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark MapType (Dict) Usage with Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d01164d-57be-4a84-9ead-7c799ee0580d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, MapType\n",
    "mapCol = MapType(StringType(), StringType(), False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca17cd5-8657-4767-a72a-088cddb3c19d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "- The first parame `keyType` is used to specify the type of key in the map\n",
    "- the second param `valueType  is use dto specify the type of the value in the map\n",
    "- Thirs param `valuContainsNull` is an optional boolean type thta is used to specify if the value of the second param can accept Null/None values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8752b250-487f-4104-bb90-ae695ae1c682",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\nNone\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\nNone\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('properties', MapType(StringType(), StringType()),True)\n",
    "])\n",
    "\n",
    "dataDictionary = [\n",
    "        ('James',{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',{'hair':'brown','eye':None}),\n",
    "        ('Robert',{'hair':'red','eye':'black'}),\n",
    "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
    "        ('Jefferson',{'hair':'brown','eye':''})\n",
    "        ]\n",
    "\n",
    "df = spark.createDataFrame(data=dataDictionary, schema=schema)\n",
    "\n",
    "print(df.printSchema())\n",
    "print(df.show(truncate=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b93d71e-956f-432d-80fe-dae96ae09d3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Access PySpark MapType Elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e286ea7a-7945-4153-9443-e63a6ca1bdfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\nNone\n"
     ]
    }
   ],
   "source": [
    "df3=df.rdd.map(lambda x: (x.name, x.properties['hair'], x.properties['eye'])).toDF(['name','hair','eye'])\n",
    "print(df3.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed31b820-6e10-4b8a-a4cc-f78307e5c52e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Let’s use another way to get the value of a key from Map using getItem() of Column type, this method takes a key as an argument and returns a value.\n",
    "\n",
    "df.withColumn('hair', df.properties.getItem('hair')) \\\n",
    "  .withColumn('eye', df.properties.getItem('eye')) \\\n",
    "   .drop('properties') \\\n",
    "    .show()\n",
    "\n",
    "df.withColumn('hair', df.properties['hair']) \\\n",
    "    .withColumn('eye', df.properties['eye']) \\\n",
    "    .drop('properties') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d370780-c837-4f33-ad1d-3aea319aaecf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n|      name| key|value|\n+----------+----+-----+\n|     James| eye|brown|\n|     James|hair|black|\n|   Michael| eye| null|\n|   Michael|hair|brown|\n|    Robert| eye|black|\n|    Robert|hair|  red|\n|Washington| eye| grey|\n|Washington|hair| grey|\n| Jefferson| eye|     |\n| Jefferson|hair|brown|\n+----------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Below are some of the MapType Functions with examples.\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df.select('name', explode('properties')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd06cee2-df75-499c-8f45-fa58eaf9750c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkByExample - 6",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
