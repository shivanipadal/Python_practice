{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c214d2d9-7179-4891-83fe-80bc233b419a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "https://www.machinelearningplus.com/pyspark/pyspark-exercises-101-pyspark-exercises-for-data-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df860b3-75c7-458c-88ef-5306ceeaed73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba7508e6-f281-438a-ba4b-0d6e71bbf795",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.master('local[*]').appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e7dd71-6e5a-4d54-b980-6d16ba15ab3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|   Name|Value|\n+-------+-----+\n|  Alice|    1|\n|    Bob|    2|\n|Charlie|    3|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([('Alice', 1), ('Bob', 2), ('Charlie', 3)], ['Name', 'Value'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56f9741-e40b-4f58-a55b-f9888c8965b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---+\n|   Name|Value|III|\n+-------+-----+---+\n|  Alice|    1|USA|\n|    Bob|    2|USA|\n|Charlie|    3|USA|\n+-------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df =df.withColumn('III', lit('USA'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be67299-672f-4b9e-af3e-1b07f48747c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "How to combine many lists to form a PySpark DataFrame?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b9ff28-a915-4d3d-93ef-02a9bd5515ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n|       _1| _2|\n+---------+---+\n|  Finance| 20|\n| Accounts| 60|\n|Marketing| 30|\n|    Sales| 40|\n|       IT| 50|\n+---------+---+\n\nOut[16]: [Row(_1='Finance', _2=20),\n Row(_1='Accounts', _2=60),\n Row(_1='Marketing', _2=30),\n Row(_1='Sales', _2=40),\n Row(_1='IT', _2=50)]"
     ]
    }
   ],
   "source": [
    "SampleDepartment = [(\"Finance\",20),(\"Accounts\", 60),(\"Marketing\",30),(\"Sales\",40),(\"IT\",50)]\n",
    "rdd = spark.sparkContext.parallelize(SampleDepartment)\n",
    "df=rdd.toDF()\n",
    "df.show()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4cdf093-1497-4002-a31f-bf2059437b00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. How to get the items of list A not present in list B?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94e4183-2585-44c7-a615-67776bda07e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: [1, 2, 3]"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd2 = spark.sparkContext.parallelize([4,5,6,7,8])\n",
    "rdd1.subtract(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c1e9428-3069-4a9d-9212-72ee11a5a26b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "5. How to get the items not common to both list A and list B?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109ca2f8-456f-4e89-b4f5-7c001044c472",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n[6, 7, 8]\nOut[24]: [1, 2, 3, 6, 7, 8]"
     ]
    }
   ],
   "source": [
    "rdd1_2 = rdd1.subtract(rdd2)\n",
    "print(rdd1_2.collect())\n",
    "rdd2_1 = rdd2.subtract(rdd1)\n",
    "print(rdd2_1.collect())\n",
    "\n",
    "rdd_not_common = rdd1_2.union(rdd2_1)\n",
    "rdd_not_common.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70a9d000-30bb-41e6-aad9-031cd89a6c5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "8. How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916a7c3e-94b8-4c33-b77a-f891de1a04c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n|name|      job|\n+----+---------+\n|John| Engineer|\n|John| Engineer|\n|Mary|Scientist|\n| Bob| Engineer|\n| Bob| Engineer|\n| Bob|Scientist|\n| Sam|   Doctor|\n+----+---------+\n\n+----+---------+\n|name|      job|\n+----+---------+\n|John| Engineer|\n|John| Engineer|\n|Mary|Scientist|\n| Bob| Engineer|\n| Bob| Engineer|\n| Bob|Scientist|\n| Sam|   Doctor|\n+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "top2_jobs = df.groupBy('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x:x).collect()\n",
    "top2_jobs\n",
    "\n",
    "df.withColumn('job', when(col('job').isin(top2_jobs), col('job')).otherwise('Other'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05648d50-8193-44a5-bcfb-fd889dcd873a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "9. How to Drop rows with NA values specific to a particular column?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710edbc5-88db-469c-89ec-031c97bed4b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n|Name|Value|  id|\n+----+-----+----+\n|   A|    1|null|\n|   B| null| 123|\n|   B|    3| 456|\n|   D| null|null|\n+----+-----+----+\n\n+----+-----+---+\n|Name|Value| id|\n+----+-----+---+\n|   B|    3|456|\n+----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.dropna(subset=['Value', 'id']).show()\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34128291-40cc-4412-9267-84d081fe5031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1 new_col1\ncol2 new_col2\ncol3 new_col3\n+--------+--------+--------+\n|new_col1|new_col2|new_col3|\n+--------+--------+--------+\n|       1|       2|       3|\n|       4|       5|       6|\n+--------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# old column names\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "# new column names\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "\n",
    "\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    print(old_name, new_name)\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d494f8e6-a52a-47f8-b710-bf8a1ecb8b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "13. How to find the numbers that are multiples of 3 from a column?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88717f7d-0c51-4f37-83a0-bb2aab0bc002",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|random|\n+---+------+\n|  0|     7|\n|  1|     9|\n|  2|     8|\n|  3|     8|\n|  4|     3|\n|  5|     1|\n|  6|     7|\n|  7|     4|\n|  8|     5|\n|  9|     1|\n+---+------+\n\n+---+------+----------------+\n| id|random|is_multiple_of_3|\n+---+------+----------------+\n|  0|     7|               0|\n|  1|     9|               1|\n|  2|     8|               0|\n|  3|     8|               0|\n|  4|     3|               1|\n|  5|     1|               0|\n|  6|     7|               0|\n|  7|     4|               0|\n|  8|     5|               0|\n|  9|     1|               0|\n+---+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, when\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "df = df.withColumn('is_multiple_of_3', when(col('random')%3 ==0, 1).otherwise(0))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d3e04c8-c22f-466d-8e23-e613f6d54836",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "14. How to extract items at given positions from a column?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874858c7-91e3-4dc0-9a87-b89cea100e21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|random|\n+---+------+\n|  0|     7|\n|  1|     9|\n|  2|     8|\n|  3|     8|\n|  4|     3|\n|  5|     1|\n|  6|     7|\n|  7|     4|\n|  8|     5|\n|  9|     1|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "pos = [0, 4, 8, 5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a12fbf8-aca1-4b39-90fd-73a358de2802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n| id|random|index|\n+---+------+-----+\n|  0|     7|    0|\n|  1|     9|    1|\n|  2|     8|    2|\n|  3|     8|    3|\n|  4|     3|    4|\n|  5|     1|    5|\n|  6|     7|    6|\n|  7|     4|    7|\n|  8|     5|    8|\n|  9|     1|    9|\n+---+------+-----+\n\n+---+------+-----+\n| id|random|index|\n+---+------+-----+\n|  0|     7|    0|\n|  4|     3|    4|\n|  5|     1|    5|\n|  8|     5|    8|\n+---+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "windows = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "df = df.withColumn('index', row_number().over(windows) - 1)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df_filtered = df.filter(df.index.isin(pos))\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0599352f-596d-4fd3-84d9-44c6b51c2654",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark machinelearningplus",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
