{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43f3681b-3635-48c4-b421-120d46751418",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark When Otherwise | SQL Case When Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac93f4d1-8777-4a17-bbc9-f516aa63a38b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af31ef2e-e6fd-4819-8409-d176645fa71e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark=SparkSession.builder \\\n",
    "      .master('local[*]') \\\n",
    "      .appName('SparkByExample') \\\n",
    "      .getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2904df0-fdaf-4464-93f1-c1311a3ffdd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n|   name|gender|salary|\n+-------+------+------+\n|  James|     M| 60000|\n|Michael|     M| 70000|\n| Robert|  null|400000|\n|  Maria|     F|500000|\n|    Jen|      |  null|\n+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf30d7aa-e6e5-49df-b0dd-35f180b626ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Using when() otherwise() on PySpark DataFrame.\n",
    "\n",
    "when() funcion take 2 parameters, first param takes a conditionand second takes a literal value or column. If condition evaluates the it returns a value from second param.\n",
    "When otherwise() not used and none of the conditions met it assigns None(null) value. Usage would be like \n",
    "`when(condition, value).otherwise(defaultvalue)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5514fa6-3449-4053-b3f5-fab1ed4b0e49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n|   name|gender|salary|new_gender|\n+-------+------+------+----------+\n|  James|     M| 60000|      Male|\n|Michael|     M| 70000|      Male|\n| Robert|  null|400000|          |\n|  Maria|     F|500000|    Female|\n|    Jen|      |  null|          |\n+-------+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df2=df.withColumn('new_gender', when(df.gender == 'M', 'Male')\n",
    "                                .when(df.gender == 'F','Female')\n",
    "                                .when(df.gender.isNull(), '')\n",
    "                                .otherwise(df.gender))\n",
    "\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4615199-fac0-4b00-932a-b027d8b6362a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Using Case When Else on DataFrame using withColumn() & select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c97ea7-1a35-442c-ba80-7685aba999fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n|name   |gender|salary|new_gender|\n+-------+------+------+----------+\n|James  |M     |60000 |Male      |\n|Michael|M     |70000 |Male      |\n|Robert |null  |400000|          |\n|Maria  |F     |500000|Female    |\n|Jen    |      |null  |          |\n+-------+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "df3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "               \"ELSE gender END\"))\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8de50a3-9974-4914-966b-ce0a91616d0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Using Case When on SQL Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343f0daf-b8e5-424d-a86a-caab92b684d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n|   name|new_gender|\n+-------+----------+\n|  James|      Male|\n|Michael|      Male|\n| Robert|          |\n|  Maria|    Female|\n|    Jen|          |\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('EMP')\n",
    "# spark.sql(\"select name, \" +\n",
    "#           \"case when gender = 'M'\" +\n",
    "#           \"then 'Male' \" +\n",
    "#           \"when gender='F' \" +\n",
    "#           \"then 'Female'\" +\n",
    "#           \"when gender is Null\" +\n",
    "#           \"then '' \" +\n",
    "#           \"ELSE gender \" +\n",
    "#           \"END as new_gender from emp\").show()\n",
    "\n",
    "spark.sql(\"select name, CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "              \"ELSE gender END as new_gender from EMP\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "727e8b1a-70ef-4334-9485-cf7218f48414",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Multiple Conditions using & and | operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4115c9-6111-4d51-9ab2-eb59c9b0ccb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df5.withColumn(“new_column”, when((col(“code”) == “a”) | (col(“code”) == “d”), “A”)\n",
    "# .when((col(“code”) == “b”) & (col(“amt”) == “4”), “B”)\n",
    "# .otherwise(“A1”)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf86e536-e999-4c62-9587-cdc14dd50129",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###PySpark SQL expr() (Expression) Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cc80a8b-143f-4496-835d-f2e67cafb218",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Concatenate Columns using || (similar to SQL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be548d8c-a36e-45e9-89da-2165bdd1097a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+\n|firstname|lastname|       Name|\n+---------+--------+-----------+\n|    james|    Bond| james,Bond|\n|    Scott|   Varsa|Scott,Varsa|\n+---------+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [('james','Bond'), ('Scott', 'Varsa')]\n",
    "df=spark.createDataFrame(data).toDF('firstname','lastname')\n",
    "df.withColumn('Name', expr(\"firstname || ',' || lastname\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1fa26dd-feaa-4909-881a-473d27f419ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Using an Existing Column Value for Expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f33085-cd7c-4d78-a239-5b4e4091ffb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n|      date|increment|  Inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import add_months\n",
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
    "df=spark.createDataFrame(data).toDF(\"date\",\"increment\") \n",
    "\n",
    "df.select(col('date'),col('increment'), expr(\"add_months(date, increment)\").alias('Inc_date')).show()\n",
    "\n",
    "\n",
    "##### Giving Column Alias along with expr()\n",
    "\n",
    "df.select(df.date, df.increment, expr(\"\"\"add_months(date,increment) as inc_date\"\"\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d031eef1-d993-4825-addb-9f3dd7006ae6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### cast Function with expr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9096e17e-d09e-4961-a37f-3688f862e8f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- increment: long (nullable = true)\n |-- str_increment: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.select('increment',expr(\"cast(increment as string) as str_increment\")).printSchema()\n",
    "# df.select(\"increment\",expr(\"cast(increment as string) as str_increment\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706c4118-90f0-4b89-b455-fd207015ff64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Arithmetic operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e9786d-178c-418e-acfe-30ed11f4d2e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n|      date|increment|new_increment|\n+----------+---------+-------------+\n|2019-01-23|        1|            6|\n|2019-06-24|        2|            7|\n|2019-09-20|        3|            8|\n+----------+---------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.date, df.increment, expr('increment + 5 as new_increment')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bef1c84f-477b-4fb6-92d6-cdec057e5c99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Using Filter with expr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce52515-2eee-47aa-9188-0051da9fb8c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n|col1|col2|\n+----+----+\n| 500| 500|\n+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Use expr()  to filter the rows\n",
    "from pyspark.sql.functions import expr\n",
    "data=[(100,2),(200,3000),(500,500)] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.filter(expr(\"col1 == col2\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fc00715-db70-46a2-9151-7caee611b133",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark lit() – Add Literal or Constant to DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9996b909-dfa1-4ca0-ac96-929a2f11a0e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\n",
    "columns= [\"EmpId\",\"Salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "707fcdf9-57ed-487e-bf16-441f2c9930a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####1. Simple usage of lit() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56e29b75-3d3a-4c08-8876-4f2e03ed67e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "df2=df.select(col('EmpId'), col('Salary'), lit('1').alias('lit_value_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "879f9131-8e2a-4b17-a615-d0c9c3578153",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####lit() function with withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e35987f0-2e0c-4bd3-968d-314a9ae393e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7653048-d8ce-4204-9d7c-6ad1c1df4fac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+----------+\n|EmpId|Salary|lit_value_1|lit_value2|\n+-----+------+-----------+----------+\n|  111| 50000|          1|      1000|\n|  222| 60000|          1|       200|\n|  333| 40000|          1|      1000|\n+-----+------+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df3=df2.withColumn('lit_value2', when((col('Salary') >=40000) & (col('Salary') < 60000), lit('1000')).otherwise(lit('200')))\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93d647f4-6bc9-43f6-9735-b88a5b82bb51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###Pyspark Split\n",
    "\n",
    "Split() function converts delimiter separated string to an array(stringType, ArrayType) column on dataframe\n",
    "\n",
    "syntax: `pyspark.sql.functions.split(str, pattern, limit = -1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2e4cbb-d91d-4d58-82c5-6252a021dc4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- dob_year: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
    "            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
    "            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
    "            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
    "            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
    "            ]\n",
    "\n",
    "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db388365-1fa6-4c00-ad9e-b07e71a422e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+------+------------------------+\n|name                |dob_year|gender|salary|Name1                   |\n+--------------------+--------+------+------+------------------------+\n|James, A, Smith     |2018    |M     |3000  |[James,  A,  Smith]     |\n|Michael, Rose, Jones|2010    |M     |4000  |[Michael,  Rose,  Jones]|\n|Robert,K,Williams   |2010    |M     |4000  |[Robert, K, Williams]   |\n|Maria,Anne,Jones    |2005    |F     |4000  |[Maria, Anne, Jones]    |\n|Jen,Mary,Brown      |2010    |      |-1    |[Jen, Mary, Brown]      |\n+--------------------+--------+------+------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df.withColumn('Name1', split(col('name'), \",\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d47c53-e063-465e-b956-d7443707bd6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Convert String to Array Column using SQL Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7927c9-e94c-4958-a620-fe472a41b1f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|           NameArray|\n+--------------------+\n| [James,  A,  Smith]|\n|[Michael,  Rose, ...|\n|[Robert, K, Willi...|\n|[Maria, Anne, Jones]|\n|  [Jen, Mary, Brown]|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('Person')\n",
    "spark.sql(\"select split(name, ',') as NameArray from person\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c301e91-306b-43f8-9af5-35fa08d7a893",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark – concat_ws(),  Convert array column to a String\n",
    "\n",
    "Syntax: `\n",
    "concat_ws(sep, *cols)\n",
    "`\n",
    "\n",
    "Which takes delimiter as a first argument and array column as second argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51f63b4-c631-4b86-92a0-6fe8cd0763f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+----------------+------------------+------------+\n|name            |languagesAtSchool |currentState|\n+----------------+------------------+------------+\n|James,,Smith    |[Java, Scala, C++]|CA          |\n|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n|Robert,,Williams|[CSharp, VB]      |NV          |\n+----------------+------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3503c106-9891-4fb9-ab5d-b5e100bceb42",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------+--------------+\n|            name| languagesAtSchool|currentState|     arrayType|\n+----------------+------------------+------------+--------------+\n|    James,,Smith|[Java, Scala, C++]|          CA|Java,Scala,C++|\n|   Michael,Rose,|[Spark, Java, C++]|          NJ|Spark,Java,C++|\n|Robert,,Williams|      [CSharp, VB]|          NV|     CSharp,VB|\n+----------------+------------------+------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, col\n",
    "\n",
    "df2=df.withColumn('arrayType', concat_ws(',', col('languagesAtSchool'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85105d29-8b7b-4633-8e82-7f5e2216a07f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###Pyspark – substring()\n",
    "\n",
    "substring() from a column\n",
    "\n",
    "syntax: `substr(str, pos, len)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e4dd26-d6a4-422a-afee-38d5c7f6c2b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-----+---+\n| id|    date|year|month|day|\n+---+--------+----+-----+---+\n|  1|20200828|2020|   08| 28|\n|  2|20180525|2018|   05| 25|\n+---+--------+----+-----+---+\n\n+--------+----+-----+---+\n|    date|year|month|day|\n+--------+----+-----+---+\n|20200828|2020|   08| 28|\n|20180525|2018|   05| 25|\n+--------+----+-----+---+\n\n+--------+----+-----+---+\n|    date|year|month|day|\n+--------+----+-----+---+\n|20200828|2020|   08| 28|\n|20180525|2018|   05| 25|\n+--------+----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "data = [(1,\"20200828\"),(2,\"20180525\")]\n",
    "columns=[\"id\",\"date\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "df.withColumn('year', substring('date',1,4)) \\\n",
    "   .withColumn('month', substring('date',5,2)) \\\n",
    "    .withColumn('day', substring('date', 7,2)).show()\n",
    "\n",
    "\n",
    "#or\n",
    "\n",
    "df.select('date', substring('date',1,4).alias('year'), \\\n",
    "                   substring('date', 5,2).alias('month'), \\\n",
    "                    substring('date', 7,2).alias('day')).show()\n",
    "\n",
    "\n",
    "#or\n",
    "\n",
    "#Using with selectExpr\n",
    "df.selectExpr('date', 'substring(date, 1,4) as year', \\\n",
    "                  'substring(date, 5,2) as month', \\\n",
    "                  'substring(date, 7,2) as day').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a95e88-ee87-4cb4-8a6c-c079e3f09261",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###PySpark – translate(), (PySpark Replace Column Values in DataFrame)\n",
    "\n",
    "We can replace column value of pyspark Dataframe by using SQL string functions regexp_replace(), translate() and overlay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3cc6246-94f5-448e-986f-bc32e1c77a84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n| id|           address|state|\n+---+------------------+-----+\n|  1|  14851 Jeffrey Rd|   DE|\n|  2|43421 Margarita St|   NY|\n|  3|  13111 Siemon Ave|   CA|\n+---+------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n",
    "    (2,\"43421 Margarita St\",\"NY\"),\n",
    "    (3,\"13111 Siemon Ave\",\"CA\")]\n",
    "df =spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "042bcec8-81ba-4be3-9c21-a2f83727194f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+------------------+\n| id|           address|state|  modified_addrees|\n+---+------------------+-----+------------------+\n|  1|  14851 Jeffrey Rd|   DE|14851 Jeffrey Road|\n|  2|43421 Margarita St|   NY|43421 Margarita St|\n|  3|  13111 Siemon Ave|   CA|  13111 Siemon Ave|\n+---+------------------+-----+------------------+\n\n+---+----------------------+-----+\n|id |address               |state|\n+---+----------------------+-----+\n|1  |14851 Jeffrey Road    |DE   |\n|2  |43421 Margarita Street|NY   |\n|3  |13111 Siemon Avenue   |CA   |\n+---+----------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import regexp_replace, when\n",
    "df.withColumn('modified_addrees', regexp_replace('address', 'Rd', 'Road')).show()\n",
    "\n",
    "##Replace Column Values Conditionally\n",
    "df.withColumn('address', \n",
    "              when(df.address.endswith('Rd'), regexp_replace(df.address, 'Rd', 'Road')) \\\n",
    "              .when(df.address.endswith('St'), regexp_replace(df.address, 'St', 'Street')) \\\n",
    "              .when(df.address.endswith('Ave'), regexp_replace(df.address, 'Ave', 'Avenue')) \\\n",
    "              .otherwise(df.address)) \\\n",
    "               .show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36c72234-a0dd-4cf3-9b14-340a0ff2f251",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Replace Column Value with Dictionary (map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a85dbdaa-2d68-474b-a98b-9d0b81eea70c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----------+\n| id|           address|     state|\n+---+------------------+----------+\n|  1|  14851 Jeffrey Rd|  Delaware|\n|  2|43421 Margarita St|  New York|\n|  3|  13111 Siemon Ave|California|\n+---+------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "stateDic={'CA':'California','NY':'New York','DE':'Delaware'}\n",
    "df2=df.rdd.map(lambda x: \n",
    "    (x.id,x.address,stateDic[x.state]) \n",
    "    ).toDF([\"id\",\"address\",\"state\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9697417-115f-4be4-a66e-2913cfa4d00d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Replace Column Value Character by Character using translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9191e4-b68a-4816-a7c6-02ad4af90f8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+-------------------+\n| id|           address|state|modified_new_column|\n+---+------------------+-----+-------------------+\n|  1|  14851 Jeffrey Rd|   DE|   A485A Jeffrey Rd|\n|  2|43421 Margarita St|   NY| 4C4BA Margarita St|\n|  3|  13111 Siemon Ave|   CA|   ACAAA Siemon Ave|\n+---+------------------+-----+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.withColumn('modified_new_column', translate(df.address, '123','ABC')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50d42b51-644d-4c68-821c-97c368c6909c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Using overlay() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a269e037-98d8-4095-8109-6124b0146ad5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|overlayed|\n+---------+\n|ABCDE_FGH|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import overlay\n",
    "df = spark.createDataFrame([(\"ABCDE_XYZ\", \"FGH\")], (\"col1\", \"col2\"))\n",
    "df.select(overlay('col1','col2', 7).alias('overlayed')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a3424a1-a900-4afb-91a9-1130e4d66e90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###PySpark to_timestamp() – Convert String to Timestamp type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b499471d-a460-47f2-9111-9f4bf4257abb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- timestamp: string (nullable = true)\n\n+---+-----------------------+---------------------+\n|id |timestamp              |Timestamp_type_column|\n+---+-----------------------+---------------------+\n|1  |2019-06-24 12:01:19.000|2019-06-24 12:01:19  |\n+---+-----------------------+---------------------+\n\n+-------------------+\n|          timestamp|\n+-------------------+\n|2019-06-24 12:01:19|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df=spark.createDataFrame(data = [('1', '2019-06-24 12:01:19.000')],\n",
    "                         schema = ['id', 'timestamp'])\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df_timestamp = df.withColumn('Timestamp_type_column', to_timestamp('timestamp'))\n",
    "df_timestamp.show(truncate=False)\n",
    "\n",
    "\n",
    "###SQL Example\n",
    "\n",
    "spark.sql(\"select to_timestamp('2019-06-24 12:01:19.000') as timestamp\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b7212dd-dd9b-4803-8cdf-459d44a2d9e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###PySpark to_date() – Convert Timestamp to Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90bc0dd8-e7ee-48da-a0cc-4648f1d4bf63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n|id |timestamp              |date_type |\n+---+-----------------------+----------+\n|1  |2019-06-24 12:01:19.000|2019-06-24|\n+---+-----------------------+----------+\n\n+---+--------------------+----------+\n| id|           timestamp| date_type|\n+---+--------------------+----------+\n|  1|2019-06-24 12:01:...|2023-09-12|\n+---+--------------------+----------+\n\n+---+--------------------+-------------------+----------+\n| id|           timestamp|                 ts|  datetype|\n+---+--------------------+-------------------+----------+\n|  1|2019-06-24 12:01:...|2019-06-24 12:01:19|2019-06-24|\n+---+--------------------+-------------------+----------+\n\n+---+-----------------------+----------+\n|id |timestamp              |date_type |\n+---+-----------------------+----------+\n|1  |2019-06-24 12:01:19.000|2019-06-24|\n+---+-----------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.withColumn('date_type', to_date('timestamp')).show(truncate=False)\n",
    "df.withColumn('date_type', to_date(current_timestamp())).show()\n",
    "\n",
    "\n",
    "#Convert TimestampType (timestamp) to DateType (date)\n",
    "df.withColumn('ts', to_timestamp('timestamp')) \\\n",
    "    .withColumn('datetype', to_date('ts')).show()\n",
    "\n",
    "#Using Column cast() Function\n",
    "df.withColumn('date_type', to_timestamp('timestamp').cast('date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae0bda8a-d5bc-4985-bd65-bfb12482858c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###PySpark date_format() – Convert Date to String format\n",
    "In PySpark use date_format() function to convert the DataFrame column from Date to String format\n",
    "syntax: `\n",
    "Syntax:  date_format(column,format)\n",
    "Example: date_format(current_timestamp(),\"yyyy MM dd\").alias(\"date_format\")\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa22ed8b-4c8d-4718-812d-efdee6292339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+-----------------+---------------------+\n|current_date|yyyy MM dd|MM/dd/yyyy hh:mm|yyyy MMMM dd     |yyyy MMMM dd E       |\n+------------+----------+----------------+-----------------+---------------------+\n|2023-09-12  |2023 09 12|09/12/2023 11:01|2023 September 12|2023 September 12 Tue|\n+------------+----------+----------------+-----------------+---------------------+\n\n+------------+----------+----------------+------------+--------------------+\n|current_date|yyyy_MM_dd|      MM_dd_yyyy|yyyy_MMMM_dd|      yyyy_MMMM_dd_E|\n+------------+----------+----------------+------------+--------------------+\n|  2023-09-12|2023 09 12|09/12/2023 11:01| 2023 Sep 12|2023 September 12...|\n+------------+----------+----------------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# df=spark.createDataFrame([], StructType([])).show()\n",
    "df=spark.createDataFrame([[\"1\"]],[\"id\"])\n",
    "df.select(current_date().alias('current_date'), \\\n",
    "    date_format(current_timestamp(), 'yyyy MM dd').alias('yyyy MM dd'), \\\n",
    "    date_format(current_timestamp(), 'MM/dd/yyyy hh:mm').alias('MM/dd/yyyy hh:mm'), \\\n",
    "    date_format(current_timestamp(), 'yyyy MMMM dd').alias('yyyy MMMM dd'), \\\n",
    "    date_format(current_timestamp(), 'yyyy MMMM dd E').alias('yyyy MMMM dd E')  \\\n",
    "    ).show(truncate=False)\n",
    "\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"select current_date() as current_date, \"+\n",
    "      \"date_format(current_timestamp(),'yyyy MM dd') as yyyy_MM_dd, \"+\n",
    "      \"date_format(current_timestamp(),'MM/dd/yyyy hh:mm') as MM_dd_yyyy, \"+\n",
    "      \"date_format(current_timestamp(),'yyyy MMM dd') as yyyy_MMMM_dd, \"+\n",
    "      \"date_format(current_timestamp(),'yyyy MMMM dd E') as yyyy_MMMM_dd_E\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6b2897b-39c8-4330-a88a-c0ce7593550c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark – Difference between two dates (days, months, years)\n",
    " datediff(), months_between()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c975e939-f395-42b4-af52-d634c2d5f0c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####datediff() Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eccfc9e6-52e2-4892-8cc7-49564181c663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "data = [(\"1\",\"2019-07-01\"),(\"2\",\"2019-06-24\"),(\"3\",\"2019-08-24\")]\n",
    "df=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb1a551-76ff-49dd-ae19-eb7f1acbfc41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------+\n| id|      date|diff in two dates|\n+---+----------+-----------------+\n|  1|2019-07-01|             1534|\n|  2|2019-06-24|             1541|\n|  3|2019-08-24|             1480|\n+---+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('diff in two dates', datediff(current_date(), col('date'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8325573-097f-4c98-9340-bac29165665b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####months_between() Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddaa611d-5421-4c52-8a98-754bb4c2cf47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n|      date|diff in months|\n+----------+--------------+\n|2019-07-01|   50.35483871|\n|2019-06-24|   50.61290323|\n|2019-08-24|   48.61290323|\n+----------+--------------+\n\n+---+----------+---------+-----------+---------------+-----------------+---------------+\n| id|      date|datesDiff|  montsDiff|montsDiff_round|        yearsDiff|yearsDiff_round|\n+---+----------+---------+-----------+---------------+-----------------+---------------+\n|  1|2019-07-01|     1534|50.35483871|          50.35|4.196236559166667|            4.2|\n|  2|2019-06-24|     1541|50.61290323|          50.61|4.217741935833334|           4.22|\n|  3|2019-08-24|     1480|48.61290323|          48.61|4.051075269166667|           4.05|\n+---+----------+---------+-----------+---------------+-----------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col('date'), months_between(current_date(), col('date')).alias('diff in months')).show()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df.withColumn(\"datesDiff\", datediff(current_date(),col(\"date\"))) \\\n",
    "  .withColumn(\"montsDiff\", months_between(current_date(),col(\"date\"))) \\\n",
    "  .withColumn(\"montsDiff_round\",round(months_between(current_date(),col(\"date\")),2)) \\\n",
    "  .withColumn(\"yearsDiff\",months_between(current_date(),col(\"date\"))/lit(12)) \\\n",
    "  .withColumn(\"yearsDiff_round\",round(months_between(current_date(),col(\"date\"))/lit(12),2)) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5370d763-f241-482e-a84f-e63000f517e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|years_diff|\n+----------+\n|      -4.2|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"select round(months_between('2019-07-01',current_date())/12,2) as years_diff\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3eb17fd-9eb6-4064-811b-75529d9c93f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkByExample -9 (PySpark Built-In Functions)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
