{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ad705c-5227-48e4-9df7-be0779be9ada",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark Groupby Explained with Example\n",
    "\n",
    "* **Syntax** :`Dataframe.groupby(*cols)`\n",
    "\n",
    "When we perform groupby() on pyspark Dataframe, it returns GroupedData object which contains below aggregate functions.\n",
    "\n",
    "count(), mean(), max(). min(), sum(), avg(), agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38cedf99-500d-4647-8433-7e5968821b52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession.builder \\\n",
    "     .master('local[*]') \\\n",
    "     .appName('SParkByExample') \\\n",
    "     .getOrCreate()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d5dfd9-2c58-4da6-a990-4e5403c092e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a95950a-1467-4fc5-a3ca-fd9fae0203b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Letâ€™s do the groupBy() on department column of DataFrame and then find the sum of salary for each department using sum() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d38a52-cdfb-4ed8-a4f5-8567a614ca84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|department|sum(salary)|\n+----------+-----------+\n|     Sales|     257000|\n|   Finance|     351000|\n| Marketing|     171000|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').sum('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97e1339-0735-4bf0-a07b-71c7823b66da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n|department|count|\n+----------+-----+\n|     Sales|    3|\n|   Finance|    4|\n| Marketing|    2|\n+----------+-----+\n\n+----------+-----------+\n|department|min(salary)|\n+----------+-----------+\n|     Sales|      81000|\n|   Finance|      79000|\n| Marketing|      80000|\n+----------+-----------+\n\n+----------+-----------+\n|department|max(salary)|\n+----------+-----------+\n|     Sales|      90000|\n|   Finance|      99000|\n| Marketing|      91000|\n+----------+-----------+\n\n+----------+-----------------+\n|department|      avg(salary)|\n+----------+-----------------+\n|     Sales|85666.66666666667|\n|   Finance|          87750.0|\n| Marketing|          85500.0|\n+----------+-----------------+\n\n+----------+-----------------+\n|department|      avg(salary)|\n+----------+-----------------+\n|     Sales|85666.66666666667|\n|   Finance|          87750.0|\n| Marketing|          85500.0|\n+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').count().show()\n",
    "df.groupBy('department').min('salary').show()\n",
    "df.groupBy('department').max('salary').show()\n",
    "df.groupBy('department').avg('salary').show()\n",
    "df.groupBy('department').mean('salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a50663-0173-4ef2-9eaa-0c65e40ca471",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### using multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "483942a0-1d56-406d-9878-528b9588800f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7211e555-3c05-42c5-be03-16054f3b492f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n|department|state|sum(salary)|sum(bonus)|\n+----------+-----+-----------+----------+\n|Sales     |NY   |176000     |30000     |\n|Finance   |NY   |162000     |34000     |\n|Marketing |NY   |91000      |21000     |\n|Sales     |CA   |81000      |23000     |\n|Finance   |CA   |189000     |47000     |\n|Marketing |CA   |80000      |18000     |\n+----------+-----+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department', 'state') \\\n",
    "    .sum('salary', 'bonus') \\\n",
    "    .orderBy(col('state').desc()) \\\n",
    "    .show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f509b1-2a65-4381-80c6-f7c0c9a93d85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Running more aggregates at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ad70aa-ff9c-454b-820d-d9c4b206fccf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n+----------+----------+-----------------+---------+---------+\n|Sales     |257000    |85666.66666666667|53000    |23000    |\n|Finance   |351000    |87750.0          |81000    |24000    |\n|Marketing |171000    |85500.0          |39000    |21000    |\n+----------+----------+-----------------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max\n",
    "\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "    ) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b26bed-6881-4fba-b4cd-4a22fc920856",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using filter on aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc66f27-0b81-486c-a067-d3b88a8f49f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n+----------+----------+-----------------+---------+---------+\n|Sales     |257000    |85666.66666666667|53000    |23000    |\n|Finance   |351000    |87750.0          |81000    |24000    |\n+----------+----------+-----------------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
    "    .where(col(\"sum_bonus\") >= 50000) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f67ebef-6e63-4537-bebb-5747f4ca1744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6365da-0f0f-4067-ba6a-dedb36da41d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark Aggregate Functions with Examples\n",
    "\n",
    "* **approx_count_distinct** : In PySpark approx_count_distinct() function returns the count of distinct items in a group.\n",
    "\n",
    "* **avg (average)** : avg() function returns the average of values in the input column.\n",
    "\n",
    "* **collect_list** : it returns all values from an input column with duplicates\n",
    "\n",
    "* **collect_set** : It returns all values from an input column with duplicate values eliminated\n",
    "\n",
    "* **countDistinct** : It returns number of disticnt elements in a columns\n",
    "\n",
    "* **count** : It returns number of elements in a column\n",
    "\n",
    "* **first** : It returns the first element in a column when ignoreNulls is set to true\n",
    "\n",
    "* **last** : It returns the last element in a column when ignoreNulls is set to true\n",
    "\n",
    "* **max** : It returns max value in column\n",
    "\n",
    "* **min** : It returns min value in column\n",
    "\n",
    "* **mean** : It returns the avg values in a column. This is alias for avg()\n",
    "\n",
    "* **sum** : It returns the sum of all values in a column\n",
    "\n",
    "* **sumDistinct** : It retuns the sum of all distinct values in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32ff3ce-335d-40a8-9dc0-117a4f9a831e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_count_distinct: [Row(approx_count_distinct(salary)=6)]\navg: 3400.0\n+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\ncollect_list: None\n+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\ncollect_set: None\ncountDistinct: 6\ncount: Row(count(salary)=10)\n+-------------+\n|first(salary)|\n+-------------+\n|3000         |\n+-------------+\n\nfirst : None\n+------------+\n|last(salary)|\n+------------+\n|4100        |\n+------------+\n\nlast : None\n+-----------+\n|max(salary)|\n+-----------+\n|4600       |\n+-----------+\n\nmax:  None\n+-----------+\n|min(salary)|\n+-----------+\n|2000       |\n+-----------+\n\nmin:  None\n+-----------+\n|avg(salary)|\n+-----------+\n|3400.0     |\n+-----------+\n\nmean None\n+-----------+\n|sum(salary)|\n+-----------+\n|34000      |\n+-----------+\n\nsum : None\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/functions.py:723: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|20900               |\n+--------------------+\n\nsumDistintct: None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct, collect_list, collect_set, countDistinct, count, first, last, mean, max, min, sum, sumDistinct\n",
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "# df.printSchema()\n",
    "# df.show(truncate=False)\n",
    "\n",
    "print(\"approx_count_distinct:\", df.select(approx_count_distinct('salary')).collect())\n",
    "\n",
    "print('avg:', df.select(avg('salary')).collect()[0][0])\n",
    "\n",
    "print('collect_list:', df.select(collect_list('salary')).show(truncate=False))\n",
    "\n",
    "print('collect_set:', df.select(collect_set('salary')).show(truncate=False))\n",
    "\n",
    "print('countDistinct:', df.select(countDistinct('salary')).collect()[0][0])\n",
    "\n",
    "print('count:', df.select(count('salary')).collect()[0])\n",
    "\n",
    "print('first :', df.select(first('salary')).show(truncate=False))\n",
    "\n",
    "print('last :', df.select(last('salary')).show(truncate=False))\n",
    "\n",
    "print('max: ', df.select(max('salary')).show(truncate=False))\n",
    "\n",
    "print('min: ', df.select(min('salary')).show(truncate=False))\n",
    "\n",
    "print('mean', df.select(mean('salary')).show(truncate=False))\n",
    "\n",
    "print('sum :', df.select(sum('salary')).show(truncate=False))\n",
    "\n",
    "print('sumDistintct:', df.select(sumDistinct('salary')).show(truncate=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08db4677-20b1-41d8-bfd6-d6e6548a1ca4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark Join Types | Join Two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a041ae-792b-4931-b0df-46fe30e88856",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema=empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cfb84cb-8e71-4c88-bd8b-6aac6406b8ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### PySpark Inner Join DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83007cc9-051a-44bf-8a74-d1f093a42779",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, how=\"inner\").orderBy(col('emp_dept_id').asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc1e5433-afa2-4118-b328-ab2f63b5788a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### PySpark Full Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243e69fd-d589-429a-88a6-a619692f7ef9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'outer' ).orderBy(col('emp_id').asc()).show(truncate=False)\n",
    "#below two commands are provide same output as above \n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\").orderBy(col('emp_id').asc()).show(truncate=False)\n",
    "   \n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\").orderBy(col('emp_id').asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac18ec2-2667-401c-beb1-b2a160ea055e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### PySpark Left and right Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c87611d9-1d5e-4563-a375-05926315e55c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"left\").show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'right').show(truncate=False)\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, 'rightouter').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e08e777-daa0-4569-bc59-8a94ccc21e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Left Semi Join\n",
    "\n",
    "returns rows from the left table where there is a match in the right table based on the join condition **but doesn't include columns from the right table**. This is similar to inner join. But in inner join returns rows from both tables where there is a match based on the join condition, **including columns from both tables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a2f540-ea68-4134-8263-e95c161c3331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"leftsemi\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e4c367-32b2-4d83-929e-5c9520a0b24c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Left Anti Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9e4ce8-80fc-4171-b0ad-49fcd44de9a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|6     |Brown|2              |2010       |50         |      |-1    |\n+------+-----+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"leftanti\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3f43682-16d3-478f-ac45-fae23cba48dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### PySpark Self Join\n",
    "Though there is no self-join type available, we can use any of the above-explained join types to join DataFrame to itself. below example use inner self join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bfd5b3c-6026-45ba-a095-e1f606eb3e84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------------+\n|emp_id|name    |superior_emp_id|superior_emp_name|\n+------+--------+---------------+-----------------+\n|2     |Rose    |1              |Smith            |\n|3     |Williams|1              |Smith            |\n|4     |Jones   |2              |Rose             |\n|5     |Brown   |2              |Rose             |\n|6     |Brown   |2              |Rose             |\n+------+--------+---------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.alias('emp1').join(empDF.alias('emp2'), \\\n",
    "      col('emp1.superior_emp_id') == col('emp2.emp_id'), 'inner') \\\n",
    "      .select(col('emp1.emp_id'), col('emp1.name'), col('emp2.emp_id').alias('superior_emp_id'), \\\n",
    "          col('emp2.name').alias('superior_emp_name')) \\\n",
    "       .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb44c195-84db-44a2-83d2-2f23a28b3759",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using SQL Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea81ee7a-bec5-4d7e-a683-248149b421e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.createOrReplaceTempView('EMP')\n",
    "deptDF.createOrReplaceTempView('DEPT')\n",
    "\n",
    "spark.sql('select * from EMP e, dept d where e.emp_dept_id == d.dept_id').show(truncate=False)\n",
    "spark.sql('select * from EMP e inner join dept d on e.emp_dept_id == d.dept_id').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c2ed71-bf9e-4fa3-a2ad-96cea6e5117e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### PySpark SQL Join on multiple DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77834363-906e-44de-b6a0-3018516341c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df1.join(df2, df1.id1 == df2.id2, 'inner') \\\n",
    "#     .join(df3, df1.id1 == df2.id3, 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c88aab-4413-4e09-b93e-3bbedbe1e1fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark RDD Tutorial | Learn with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0237b0a-d85f-41e5-8403-a9d444d8205d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Create RDD from parallelize    \n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "#Create RDD from external Data source\n",
    "# rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")\n",
    "\n",
    "# Create RDD using sparkContext.wholeTextFiles()\n",
    "# wholeTextFiles() function returns a PairRDD with the key being the file path and value being file content.\n",
    "#Reads entire file into a RDD as single record.\n",
    "rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e96e3de-ac7e-4ed2-9135-88884811eae5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create empty RDD using sparkContext.emptyRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe2e5ce-da35-4a29-9557-14ea97c36dad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creates empty RDD with no partition    \n",
    "rdd = spark.sparkContext.emptyRDD \n",
    "# rddString = spark.sparkContext.emptyRDD[String]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65edd119-4eb4-43b4-a594-9d59c3f92aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73e1b1ef-6b2c-443c-b134-e7b45ac78479",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* **RDD Parallelize** : When we use parelleize() or textfile() or wholeTextFiles() methods of SparkContext to initiate RDD, it automatically splits the data into partitions based on resource availability. When you run it on laptop it would create partitions as the same number of cores available on your system. \n",
    "\n",
    "* **getNumPartitions**: This RDD function which returns a number of partitions our dataset splits into. \n",
    "syntax is \n",
    "\n",
    "`print(\"initial partition count:\"+str(rdd.getNumPartitions()))\n",
    "#Outputs: initial partition count:2\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cab2497-6ca2-4944-9108-78e1dc379f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Repartition and Coalesce\n",
    "\n",
    "PySpark provides two ways to repartition; \n",
    "  1. repartition(): Which shuffles data from all nodes also called full shuffle. It is a very **expansive operation** as it shuffles data from all nodes in a cluster\n",
    "  2. coalesce() : Which shuffles data from minimum nodes . ex: if you have data in 4 partitions and doing coalesce(2) moves data from just 2 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbf245c-3904-45b3-b3c4-4c75e7e62500",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-partition count:4\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "reparRdd = rdd.repartition(4)\n",
    "print(\"re-partition count:\"+str(reparRdd.getNumPartitions()))\n",
    "#Outputs: \"re-partition count:4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c68707cc-d2a1-4053-a581-9f9ce21fca63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### RDD Transformations with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ce99599-30d5-4abf-8e24-68d8445b0066",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile('/FileStore/tables/test.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5c1dfb6-00b5-4797-bdc8-0d34b799abb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* **flatmap()** : it is a transformation operation in spark that is used to transform each element of a collection into zero or more elements and then flatten the results into single output collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2263aeee-dc5d-4b0d-aa5c-e67ab24191b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "# rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b59567b1-4bc0-4eb5-a329-951b898fe517",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* **map()** : is a transformation operation that is used to apply a function to each element of an RDD or DataFrame. It produces a new RDD or DataFrame where each element is the result of applying the specified function to the correspanding element of the original RDD or DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745556c5-7715-4a6a-958b-ce80e0b8fe69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda x: (x,1))\n",
    "# rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef02c94b-7f22-41b8-b30f-768a4ee6b140",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* **reduceByKey()**: Is a transformation operation that is typically used on a **pair of RDD** to perform a reduction operation on elements with the same key. It groups elements by their keys and applies a specific function to reduce the value associated with each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13219cae-77e2-49e5-888a-70fc81adfd4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd4 = rdd3.reduceByKey(lambda a,b:a+b)\n",
    "# rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151e0130-d1d8-41a6-8d41-41170fa81690",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* **sortByKey()**: is a transformation operation that is used to sort the elements of a pair RDD by their keys in ascending or descending order. It arranges the key-value pairs in a specified order based on the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf8f273-db42-4b34-84ac-53538d42c561",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd5=rdd4.map(lambda x:(x[1],x[0])).sortByKey(ascending=True)\n",
    "# rdd5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7954cc68-8d4c-4c23-a476-28a1fa20463e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* **filter**: is a transformation is used to filter the records in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f19fc67c-9b46-495c-a2af-20306e318a9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[118]: [(18, 'Wonderland'), (27, 'anyone'), (27, 'anywhere'), (27, 'and')]"
     ]
    }
   ],
   "source": [
    "rdd4 = rdd5.filter(lambda x: 'an' in x[1])\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb3f75ab-9733-49db-9325-5ddda98ba76b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### RDD Actions with example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4375fc4-f549-4143-9bb2-6503f6d73566",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* **count()** â€“ Returns the number of records in an RDD\n",
    "\n",
    "* **first()** - Returns the fist record\n",
    "\n",
    "* **last()** - Returns the last record\n",
    "\n",
    "* **max()** - Returns the max record\n",
    "\n",
    "* **reduce()** - Reduces the records to single, we can use this count or sum\n",
    "\n",
    "* **take()** - Returns the record specified as an argumenent\n",
    "\n",
    "* **collect()** - Returns all data from RDD as an array. Be careful when you use this action when you are working with huge RDD with millions and billions of data as you may run out of memory on the driver\n",
    "\n",
    "* **saveAsTextFile()** - we can write RDD to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29df3b73-df09-4d8d-91e1-32fa9dbf018e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 23\nfirst_element 1\nmax: 6\nreduce: 21\ntake: [1, 2, 3]\ncollect: [1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"count:\", rdd5.count()) ## count\n",
    "#######first###\n",
    "rdd_f = spark.sparkContext.parallelize([1,2,3,4,5,6])\n",
    "print(\"first_element\", rdd_f.first())\n",
    "print(\"max:\" ,rdd_f.max())\n",
    "\n",
    "######reduce()##########\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "print(\"reduce:\",rdd_f.reduce(add))\n",
    "\n",
    "#######take(n), return first n elements\n",
    "print(\"take:\", rdd_f.take(3))\n",
    "\n",
    "#collect to retrieve all elements from the RDD\n",
    "print(\"collect:\", rdd_f.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7b4c015-5301-4d93-b60d-f9fd1be5cf73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkByExample -2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
