{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba01963-f0bb-4d7f-8103-89434f3ebc52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f5d1f2-8b65-4f52-b834-3fad39c4b8c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d413fa65-1525-45af-bc86-dfb6ee506c63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n|firstname|middlename|lastname|       dob|gender|salary|\n+---------+----------+--------+----------+------+------+\n|    James|          |   Smith|1991-04-01|     M|  3000|\n|  Michael|      Rose|        |2000-05-19|     M|  4000|\n|   Robert|          |Williams|1978-09-05|     M|  4000|\n|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n+---------+----------+--------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58c72159-a2ac-4370-8253-84b9f7e4c108",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### RDD\n",
    "\n",
    "In order to create an RDD, first, you need to create a SparkSession which is an entry point to the PySpark application. SparkSession can be created using a builder() or newSession() methods of the SparkSession.\n",
    "\n",
    "Spark session internally creates a sparkContext variable of SparkContext. You can create multiple SparkSession objects but only one SparkContext per JVM. In case if you want to create another new SparkContext you should stop existing Sparkcontext (using stop()) before creating a new one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80ac055-d4de-4ff6-9be6-1478171ada3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = ['language', 'users_count']\n",
    "data = [('java', '2000'),('Python', '10000'),('Scala', '3000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932a7f9a-78ee-4122-bc22-91ffa5e243a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder  \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('SparkByExamples.com') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68edc32b-765c-4be1-a048-63eb3575d0bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c548c93-a98a-46bf-8905-8e3e5d94c5a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Create a DF from an RDD\n",
    "\n",
    "##### a. Using to DF Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965eadf1-ce7d-4e73-9f02-0133dd43db16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD = rdd.toDF(columns)\n",
    "dfFromRDD.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f17d9936-c227-4af9-848e-f3fa8411b9d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### b. using createDataFrame from SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f545472-a5b7-46d4-8e0a-053cf3dd1fb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e32ca2e4-d145-4004-9e8a-fdef7809a186",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Create DataFrame from List collection\n",
    "\n",
    "#### a. createDataFrame() from sparkSession\n",
    "It takes list of objects as an argument and chain with toDF() to specify the names to columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0499224-bfff-4ce3-ac48-3c306db5f6d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfFromData2 = spark.createDataFrame(data).toDF(*columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f50f0a81-5bd1-4277-933d-dcde8503a662",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### b. using createDataFrame() with the Row type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f84c780a-cb7c-4f19-8934-0ac624119d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "rowData = map(lambda x: Row(*x), data)\n",
    "dfFromData3 = spark.createDataFrame(rowData, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0bf75fd-fcc9-4495-ae08-4d10e119324f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### c. createDataFrame() with schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60eaffb8-aca1-4ba0-aa3f-861bcd910015",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([   \n",
    "                     StructField('firstname', StringType(), True),  ## Name, DataType, Nullable\n",
    "                     StructField('middlename', StringType(), True),\n",
    "                     StructField('lastname', StringType(), True),\n",
    "                     StructField('id', StringType(), True),\n",
    "                     StructField('gender', StringType(), True),\n",
    "                     StructField('salary', StringType(), True)\n",
    "                     ])\n",
    "\n",
    "df = spark.createDataFrame(data=data2, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfa8daa5-4b10-4a99-8b6a-9dd4a322d67b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Create DataFrame from Data sources \n",
    "#### a.Creating Dataframe from csv\n",
    "df2 = spark.read.csv('file.csv')\n",
    "#### b. creating from text file \n",
    "df2 = spark.read.csv('file.txt')\n",
    "#### c. creating from json file\n",
    "df2 = spark.read.json('file.json)\n",
    "#### d. Other sources (Avro, Parquet, ORC, Kafka)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efaa8d62-2a7d-437e-a423-80d1bb7dd81a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark withColumnRenamed to Rename Column on DataFrame\n",
    "Since Dataframes are immutable collection, we can't rename or update a column instaed when using withColumnRenamed() it creates a new Dataframw with updated column name. \n",
    "\n",
    "Syntax is \n",
    "\n",
    "`withColumnRenamed(existingName, newName)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec677d57-125a-4b2e-b940-64f88ca9379f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
    "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
    "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
    "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
    "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfd21838-20f0-498e-a98b-cb7cb9e60df6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### a. To rename Dataframe column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088422f4-2b78-4382-8342-66d070791b1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: ['name', 'dob', 'gender', 'salary']"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727bfa05-7c28-4920-b569-9a84cddaab3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('dob', 'DateOfBirth').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9779e858-0f72-4464-a639-ac77a8b7946e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####b. To rename multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff2ba975-5fca-484c-9f49-49d4f5575795",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary_amount: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumnRenamed('dob', 'DateOfBirth') \\\n",
    "        .withColumnRenamed('salary', 'salary_amount') \n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d10d105d-a7e6-43a7-a0c4-33a043419c11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### c. using pyspark structtype - To rename a nested column in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "917fd8ef-2faf-4623-81e9-78cc3aa0c941",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- mname: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "schema2 = StructType([\n",
    "    StructField('fname', StringType()),\n",
    "    StructField('mname', StringType()),\n",
    "    StructField('lname', StringType())\n",
    "])\n",
    "\n",
    "df.select(col('name').cast(schema2), \\\n",
    "    col('dob'), col('gender'), col('salary')\n",
    "    ).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6b3c84-14f0-4c65-8e78-8cbdc466e0eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### d. Using Select – To rename nested elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b44b1f-e85e-44e2-b04b-4eba674be65f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col('name.firstname').alias('fname'), \\\n",
    "          col('name.middlename').alias('mname'), \\\n",
    "          col('name.lastname').alias('lname'), \\\n",
    "           col('dob'), col('gender'), col('salary')) \\\n",
    "           .printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42b63e97-21cd-41da-93e9-1f18f9ea6328",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### e. Using PySpark DataFrame withColumn – To rename nested columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3123c42-6acb-4300-877c-a3b4d25a45a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df4 = df.withColumn('fname', col('name.firstname')) \\\n",
    "         .withColumn('mname', col('name.middlename')) \\\n",
    "         .withColumn('lname', col('name.lastname')) \\\n",
    "         .drop(\"name\")\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3652a728-cde1-46fc-a7d0-595ee1e49d2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### f. Using toDF() - To Change all columns in a Pyspark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01d6b33-ab39-4649-bcb7-38fedba65a2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- newCol1: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- newCol2: string (nullable = true)\n |-- newCol3: string (nullable = true)\n |-- newCol4: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "newColumns = ['newCol1', 'newCol2', 'newCol3', 'newCol4']\n",
    "df.toDF(*newColumns).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "316eb53e-17f0-48b5-bb97-62e87aa42f20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### PySpark withColumn() Usage with Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c51580-15b6-41fe-9649-e9095dd27402",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fdf974f-bcf4-4524-84f3-8d51a8d6fa33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### a. Change DataType using withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "999c62b2-fbe1-433b-b9a7-6a54867871db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n|firstname|middlename|lastname|       dob|gender|salary|\n+---------+----------+--------+----------+------+------+\n|    James|          |   Smith|1991-04-01|     M|  3000|\n|  Michael|      Rose|        |2000-05-19|     M|  4000|\n|   Robert|          |Williams|1978-09-05|     M|  4000|\n|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n+---------+----------+--------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('salary',col('salary').cast('Integer')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ada24de4-3367-4b6f-ba28-5f20e30bdc98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####b. Update the value of an existing column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0fef9a1-12bd-4558-b801-8ba4f8d254dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n|firstname|middlename|lastname|       dob|gender|salary|\n+---------+----------+--------+----------+------+------+\n|    James|          |   Smith|1991-04-01|     M|300000|\n|  Michael|      Rose|        |2000-05-19|     M|400000|\n|   Robert|          |Williams|1978-09-05|     M|400000|\n|    Maria|      Anne|   Jones|1967-12-01|     F|400000|\n|      Jen|      Mary|   Brown|1980-02-17|     F|  -100|\n+---------+----------+--------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('salary', col('salary')*100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be53c3fd-6153-43f7-80f0-05b6e945b226",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### c. Create a column from an existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aefcf060-73d7-4836-a3d3-81fd22138822",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+------------+\n|firstname|middlename|lastname|       dob|gender|salary|CopiedColumn|\n+---------+----------+--------+----------+------+------+------------+\n|    James|          |   Smith|1991-04-01|     M|  3000|       -3000|\n|  Michael|      Rose|        |2000-05-19|     M|  4000|       -4000|\n|   Robert|          |Williams|1978-09-05|     M|  4000|       -4000|\n|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|       -4000|\n|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           1|\n+---------+----------+--------+----------+------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('CopiedColumn',col('salary')* -1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1733a6a5-10b7-425b-bc80-dac52896ce2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####d. add a new column using withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4552dc44-caee-4a49-9709-07574032bc4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-------+\n|firstname|middlename|lastname|       dob|gender|salary|Country|\n+---------+----------+--------+----------+------+------+-------+\n|    James|          |   Smith|1991-04-01|     M|  3000|    USA|\n|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA|\n|   Robert|          |Williams|1978-09-05|     M|  4000|    USA|\n|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA|\n|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA|\n+---------+----------+--------+----------+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "df.withColumn('Country',f.lit('USA')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a0ec1e-7607-4449-a33c-013ee892ae3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-------+-------------+\n|firstname|middlename|lastname|       dob|gender|salary|Country|anotherColumn|\n+---------+----------+--------+----------+------+------+-------+-------------+\n|    James|          |   Smith|1991-04-01|     M|  3000|    USA| anotherValue|\n|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA| anotherValue|\n|   Robert|          |Williams|1978-09-05|     M|  4000|    USA| anotherValue|\n|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA| anotherValue|\n|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA| anotherValue|\n+---------+----------+--------+----------+------+------+-------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Country', f.lit('USA')) \\\n",
    "    .withColumn('anotherColumn', f.lit('anotherValue')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e18765c-e0c7-4982-9029-c434a62dc328",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark Where Filter Function | Multiple Conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0e4017-96c4-4e96-8ad4-1154d18fc65a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- languages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType,StructField , StringType, IntegerType, ArrayType\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    " ]\n",
    "        \n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    " ])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3bb8d15-009e-4046-8be7-4a3f94bca882",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### a. DataFrame filter() with Column Condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f8fc2a-ff15-4e6a-9be5-b0e350a92b66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|name                |languages         |state|gender|\n+--------------------+------------------+-----+------+\n|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|name                |languages         |state|gender|\n+--------------------+------------------+-----+------+\n|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n+--------------------+------------------+-----+------+\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "#using equals condition\n",
    "\n",
    "df.filter(df.state == 'OH').show(truncate=False)\n",
    "\n",
    "#not equals condition\n",
    "df.filter(df.state != 'OH').show(truncate=False)\n",
    "df.filter(~(df.state == 'OH')).show(truncate=False) \n",
    "\n",
    "#using sql col function\n",
    "df.filter(col('state') == 'OH').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cad3adb-17e6-4472-8aeb-e97b84ab6d80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### c. filter() with SQL Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a58f6c62-ccf2-4dbc-b34e-4566daf2ef66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+-------------------+------------------+-----+------+\n|               name|         languages|state|gender|\n+-------------------+------------------+-----+------+\n|     {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n+-------------------+------------------+-----+------+\n\n+-------------------+------------------+-----+------+\n|               name|         languages|state|gender|\n+-------------------+------------------+-----+------+\n|     {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n+-------------------+------------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Using SQL Expression\n",
    "df.filter(\"gender == 'M'\").show()\n",
    "#For not equal\n",
    "df.filter(\"gender != 'M'\").show()\n",
    "df.filter(\"gender <> 'M'\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e7fc8a-a2ca-44a6-9f24-10f747fad447",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####d. Filter with Multiple Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac128da-0e0d-4002-b488-0ddd718648e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.state == 'OH') & (df.gender == 'M')) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09d3e1be-524b-4546-8b6e-8a7177b316d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####e. filter() based on list values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d1fcf9-a71a-44ab-889e-c72404c94f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n+--------------------+------------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "li = ['OH','CA', 'DE']\n",
    "df.filter(df.state.isin(li)).show()\n",
    "df.filter(df.state.isin(li)).show(truncate=False)\n",
    "#These show all records with NY (NY is not part of the list)\n",
    "df.filter(~df.state.isin(li)).show() \n",
    "df.filter(df.state.isin(li) == False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9656cd2-02c7-4c94-94d2-b560994acaa2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####f. Filter Based on Starts With, Ends With, Contains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d7f969-4658-44d9-8da1-53cd1f6edd09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.startswith('N')).show()\n",
    "df.filter(df.state.endswith('H')).show()\n",
    "df.filter(df.state.contains('H')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4352d8d7-202e-41ab-8f6e-d92c71a71e6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### g.  PySpark Filter like and rlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415a6c11-8adc-40d6-8342-1a4b6043d823",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n| id|      name|\n+---+----------+\n|  4|Rames rose|\n+---+----------+\n\n+---+-----------+\n| id|       name|\n+---+-----------+\n|  2|Michel Rose|\n|  3| Rames Rose|\n|  4| Rames rose|\n+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data2 = [(2, \"Michel Rose\"),(3, \"Robert williams\"),(3, \"Rames Rose\"),(4, \"Rames rose\")]\n",
    "df2=spark.createDataFrame(data = data2, schema = ['id','name'])\n",
    "\n",
    "#like - sql LIKE pattern \n",
    "df2.filter(df2.name.like('%rose%')).show()\n",
    "\n",
    "df2.filter(df2.name.rlike(\"(?i)rose\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab90e72-4e3a-4426-a0ff-0f5eca883419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####h. Filter on an Array column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e580e3-fb64-42fb-8934-0f181894bbd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+------+\n|name            |languages         |state|gender|\n+----------------+------------------+-----+------+\n|{James, , Smith}|[Java, Scala, C++]|OH   |M     |\n|{Anna, Rose, }  |[Spark, Java, C++]|NY   |F     |\n+----------------+------------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df.filter(array_contains(df.languages, \"Java\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5730e5d1-73e3-4ccb-b6b9-1348ea7c0a46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####i. Filtering on Nested Struct columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add39e01-44f1-40c8-ba77-716391c23df1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------+-----+------+\n|name                  |languages   |state|gender|\n+----------------------+------------+-----+------+\n|{Julia, , Williams}   |[CSharp, VB]|OH   |F     |\n|{Mike, Mary, Williams}|[Python, VB]|OH   |M     |\n+----------------------+------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.lastname == 'Williams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "691d01e2-7793-4f9c-80ce-863fb4aaae32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark orderBy() and sort() explained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516b89d7-5431-46c9-b15f-71860be6fd31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54aec20b-0c82-4db8-b8cc-e2d9cc3940b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### a. DataFrame sorting using the sort() function\n",
    "\n",
    "syntax is \n",
    "`\n",
    "sort(self, *cols, **kwargs):\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61a2cbf-a9b6-4b5d-9905-07ab21d43db8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort('salary').show()\n",
    "df.sort('department','state').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dc6b640-2d23-48d2-955d-e64fbf3fa4cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####b. DataFrame sorting using orderBy() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2206fff6-1625-4c93-8fcb-73cd5a8b8b06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|        James|     Sales|   NY| 90000| 34|10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('salary').show()\n",
    "df.orderBy('department','state').show()\n",
    "df.orderBy(col('department'),col('state')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25611254-03be-40a9-b946-d0db6e0de210",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####c. Sort by Ascending (ASC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90608334-f64c-4ce2-b9e2-60fa361e5017",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|        James|     Sales|   NY| 90000| 34|10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.department.asc(), df.state.asc()).show(truncate=False)\n",
    "df.sort(col('department').asc(), col('state').asc()).show()\n",
    "df.orderBy(col('department').asc(), col('state').asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dca2cec-1fab-4a5e-a708-cb90c4fb60fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####d. Sort by Descending (DESC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9122f52-8c1e-41dc-96a8-a4060f859e88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.department.asc(), df.state.desc()).show()\n",
    "df.sort(col('department').asc(), col('state').desc()).show()\n",
    "df.orderBy(col('department').asc(), col('state').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72ba8807-40e3-4173-b73d-ef16cca8832e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### e. Using Raw SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b51b2d2-0ebe-4efa-a8dc-515f5f3268bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('EMP')\n",
    "spark.sql(\"select * from EMP order by  department desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca5d19c-905b-4914-a239-c93f198a00a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark Explode Array and Map Columns to Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36c774f3-93c2-46e6-9570-67f747d28b67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###I. explode() – PySpark explode array or map column to rows\n",
    " `explode`:\n",
    "   - `explode` is used to transform an array or map column into multiple rows, where each element of the array or map becomes a separate row in the resulting DataFrame.\n",
    "   - If the column being exploded contains null values, `explode` will simply omit those null values(`None`) and generate rows for the non-null elements.\n",
    "   - It does not create any new rows with null values(`None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5d4d0d-3041-43ea-b94b-b3273de829c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- knownLanguages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-------------------+--------------------+\n|      name|     knownLanguages|          properties|\n+----------+-------------------+--------------------+\n|     James|      [Java, Scala]|{eye -> brown, ha...|\n|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n|Washington|               null|                null|\n| Jefferson|             [1, 2]|                  {}|\n+----------+-------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
    "\n",
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "223bbd7f-dc71-4c3d-bf3b-c87e59190d6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####a. explode – array column example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc283bc-b45d-4f0b-b2ce-00f3d55cbd9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n|     name|   col|\n+---------+------+\n|    James|  Java|\n|    James| Scala|\n|  Michael| Spark|\n|  Michael|  Java|\n|  Michael|  null|\n|   Robert|CSharp|\n|   Robert|      |\n|Jefferson|     1|\n|Jefferson|     2|\n+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name, explode(df.knownLanguages)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9acdde47-bd6b-44c9-bd20-fa41a37a1710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####b. explode – map column example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dff522e-71eb-444f-ba51-7f70e9ff80d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n|   name| key|value|\n+-------+----+-----+\n|  James| eye|brown|\n|  James|hair|black|\n|Michael| eye| null|\n|Michael|hair|brown|\n| Robert| eye|     |\n| Robert|hair|  red|\n+-------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,explode(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65711a9c-a943-44b8-8a5d-2b43768d6e1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###II. explode_outer() – Create rows for each element in an array or map.\n",
    "\n",
    "`explode_outer`:\n",
    "   - `explode_outer` also transforms an array or map column into multiple rows, similar to `explode`.\n",
    "   - However, unlike `explode`, `explode_outer` includes rows for null values in the column being exploded. It generates a separate row with a null value for each null element in the array or map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c91da6-6d97-47cb-8090-564a3101d1d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n|      name|   col|\n+----------+------+\n|     James|  Java|\n|     James| Scala|\n|   Michael| Spark|\n|   Michael|  Java|\n|   Michael|  null|\n|    Robert|CSharp|\n|    Robert|      |\n|Washington|  null|\n| Jefferson|     1|\n| Jefferson|     2|\n+----------+------+\n\n+----------+----+-----+\n|      name| key|value|\n+----------+----+-----+\n|     James| eye|brown|\n|     James|hair|black|\n|   Michael| eye| null|\n|   Michael|hair|brown|\n|    Robert| eye|     |\n|    Robert|hair|  red|\n|Washington|null| null|\n| Jefferson|null| null|\n+----------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,explode_outer(df.knownLanguages)).show()\n",
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,explode_outer(df.properties)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b351b1d4-0090-457f-8ab6-0aed891830df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explode_outer O/P:\n+-------+------+\n|   name| fruit|\n+-------+------+\n|  Alice| apple|\n|  Alice|banana|\n|    Bob|cherry|\n|    Bob|  null|\n|Charlie|  null|\n+-------+------+\n\nExplode O/P:\n+-----+------+\n| name| fruit|\n+-----+------+\n|Alice| apple|\n|Alice|banana|\n|  Bob|cherry|\n|  Bob|  null|\n+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode_outer, explode\n",
    "data = [(\"Alice\", [\"apple\", \"banana\"]), \n",
    "        (\"Bob\", [\"cherry\", None]), \n",
    "        (\"Charlie\", None)]\n",
    "\n",
    "columns = [\"name\", \"fruits\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Explode_outer O/P:\")\n",
    "df.select(\"name\", explode_outer(\"fruits\").alias(\"fruit\")).show()\n",
    "print(\"Explode O/P:\")\n",
    "df.select(\"name\", explode(\"fruits\").alias(\"fruit\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72899629-ebf3-4d9e-8dff-4ae9827389ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### III. posexplode and posexplode_outer()\n",
    "\n",
    "`posexplode` and `posexplode_outer` are used to explode arrays or maps while also providing the position (index) of each element in the resulting DataFrame. These functions are similar to `explode` and `explode_outer`, but they include an additional column that indicates the position of the exploded element within the original array or map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7acad098-0d8f-4646-9fb0-45842fbf6c39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posexplode o/p:\n+---------+---+------+\n|     name|pos|   col|\n+---------+---+------+\n|    James|  0|  Java|\n|    James|  1| Scala|\n|  Michael|  0| Spark|\n|  Michael|  1|  Java|\n|  Michael|  2|  null|\n|   Robert|  0|CSharp|\n|   Robert|  1|      |\n|Jefferson|  0|     1|\n|Jefferson|  1|     2|\n+---------+---+------+\n\n+-------+---+----+-----+\n|   name|pos| key|value|\n+-------+---+----+-----+\n|  James|  0| eye|brown|\n|  James|  1|hair|black|\n|Michael|  0| eye| null|\n|Michael|  1|hair|brown|\n| Robert|  0| eye|     |\n| Robert|  1|hair|  red|\n+-------+---+----+-----+\n\nposexplode_outer o/p:\n+----------+----+------+\n|      name| pos|   col|\n+----------+----+------+\n|     James|   0|  Java|\n|     James|   1| Scala|\n|   Michael|   0| Spark|\n|   Michael|   1|  Java|\n|   Michael|   2|  null|\n|    Robert|   0|CSharp|\n|    Robert|   1|      |\n|Washington|null|  null|\n| Jefferson|   0|     1|\n| Jefferson|   1|     2|\n+----------+----+------+\n\n+----------+----+----+-----+\n|      name| pos| key|value|\n+----------+----+----+-----+\n|     James|   0| eye|brown|\n|     James|   1|hair|black|\n|   Michael|   0| eye| null|\n|   Michael|   1|hair|brown|\n|    Robert|   0| eye|     |\n|    Robert|   1|hair|  red|\n|Washington|null|null| null|\n| Jefferson|null|null| null|\n+----------+----+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import posexplode_outer, posexplode\n",
    "\n",
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "#posexplode\n",
    "\n",
    "print(\"posexplode o/p:\")\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,posexplode(df.knownLanguages)).show()\n",
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,posexplode(df.properties)).show()\n",
    "\n",
    "#posexplode_outer\n",
    "\n",
    "print(\"posexplode_outer o/p:\")\n",
    "from pyspark.sql.functions import posexplode_outer\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,posexplode_outer(df.knownLanguages)).show()\n",
    "\n",
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,posexplode_outer(df.properties)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f71bfdb-cb2e-40d3-b4de-e8e13ea0156e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark – explode nested array into rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b907c56-7ad4-49ce-b184-e7eca18a28a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- subjects: array (nullable = true)\n |    |-- element: array (containsNull = true)\n |    |    |-- element: string (containsNull = true)\n\n+-------+-----------------------------------+\n|name   |subjects                           |\n+-------+-----------------------------------+\n|James  |[[Java, Scala, C++], [Spark, Java]]|\n|Michael|[[Spark, Java, C++], [Spark, Java]]|\n|Robert |[[CSharp, VB], [Spark, Python]]    |\n+-------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "arrayArrayData = [\n",
    "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f4b8a9-de00-4c46-9b53-d53261675fd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n|   name|               col|\n+-------+------------------+\n|  James|[Java, Scala, C++]|\n|  James|     [Spark, Java]|\n|Michael|[Spark, Java, C++]|\n|Michael|     [Spark, Java]|\n| Robert|      [CSharp, VB]|\n| Robert|   [Spark, Python]|\n+-------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,explode(df.subjects)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d3b0502-ec54-495f-a136-d573d0253534",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### flatten : \n",
    "If you want to flatten the arrays, use flatten function which converts array of array columns to a single array on DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c254512e-c809-4e27-9591-42edac38a04f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+\n|name   |flatten(subjects)              |\n+-------+-------------------------------+\n|James  |[Java, Scala, C++, Spark, Java]|\n|Michael|[Spark, Java, C++, Spark, Java]|\n|Robert |[CSharp, VB, Spark, Python]    |\n+-------+-------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import flatten\n",
    "\n",
    "df.select(df.name, flatten(df.subjects)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6bec72-15e2-4b29-b13f-921cc968c461",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark Read CSV file into DataFrame\n",
    "\n",
    "* **Read Multiple CSV Files** : Using the read.csv() method you can also read multiple csv files, just pass all file names by separating comma as a path, for example : \n",
    "`\n",
    "df = spark.read.csv(\"path1,path2,path3\")\n",
    "`\n",
    "* **Read all CSV Files in a Directory** : We can read all CSV files from a directory into DataFrame just by passing directory as a path to the csv() method. \n",
    "`\n",
    "df = spark.read.csv(\"Folder path\")\n",
    "`\n",
    "* **delimiter** : \n",
    "`\n",
    "df3 = spark.read.options(delimiter=',').csv(\"C:/apps/sparkbyexamples/src/pyspark-examples/resources/zipcodes.csv\")\n",
    "`\n",
    "* **inferSchema** :`\n",
    "df4 = spark.read.options(inferSchema='True',delimiter=',').csv(\"src/main/resources/zipcodes.csv\")\n",
    "`\n",
    "* **header** :`df3 = spark.read.options(header='True', inferSchema='True', delimiter=',')csv(\"/tmp/resources/zipcodes.csv\")`\n",
    "\n",
    "* **Write PySpark DataFrame to CSV file** : Use the write() method of the PySpark DataFrameWriter object to write PySpark DataFrame to a CSV file.\n",
    "\n",
    "  - **Options**: `df2.write.options(header='True', delimiter=',').csv(\"/tmp/spark_output/zipcodes\")`\n",
    "  - **Saving modes**: PySpark DataFrameWriter also has a method mode() to specify saving mode.\n",
    "\n",
    "        `overwrite`: mode is used to overwrite the existing file.\n",
    "\n",
    "        `append` – To add the data to the existing file.\n",
    "\n",
    "        `ignore` – Ignores write operation when the file already exists.\n",
    "\n",
    "        `error` – This is a default option when the file already exists, it returns an error.\n",
    "\n",
    "   `df2.write.mode('overwrite').csv(\"/tmp/spark_output/zipcodes\")\n",
    "                   (or)\n",
    "    df2.write.format(\"csv\").mode('overwrite').save(\"/tmp/spark_output/zipcodes\")`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2b1d5e1-5b24-4476-a490-f4a119915f38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkByExamples",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
