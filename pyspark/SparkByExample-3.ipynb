{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef0e0af-1356-4657-9544-01e4f2cc1f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark Broadcast Variables\n",
    "\n",
    "Broadcast variables not sent to excutors with sc.broadcast(variable) call instead, they will be sent to executors when they are first used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2823e55-ce62-4f4f-a76c-f8d962b2b4df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('SparkByExample').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807bd54e-1f7f-45f9-8f5a-2328121ab2e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('james', 'smith', 'usa', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'William', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "source": [
    "# PySpark RDD Broadcast variable example\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "data = [('james','smith', 'usa','CA'), ('Michael','Rose','USA','NY'),('Robert','William', 'USA','CA'),('Maria','Jones','USA','FL')]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x:(x[0], x[1], x[2], state_convert(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0077dfcf-f03f-4b35-9aae-be7ea09297bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-503504833156609>:18\u001B[0m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstate_convert\u001B[39m(code):\n",
       "\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m broadcastStates\u001B[38;5;241m.\u001B[39mvalue[code]\n",
       "\u001B[0;32m---> 18\u001B[0m result \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconvertedstate\u001B[39m\u001B[38;5;124m'\u001B[39m, when(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39misin(\u001B[38;5;28mlist\u001B[39m(states\u001B[38;5;241m.\u001B[39mkeys())),lit(states[col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m'\u001B[39m)]))\u001B[38;5;241m.\u001B[39motherwise(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'lit' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-503504833156609>:18\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstate_convert\u001B[39m(code):\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m broadcastStates\u001B[38;5;241m.\u001B[39mvalue[code]\n\u001B[0;32m---> 18\u001B[0m result \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconvertedstate\u001B[39m\u001B[38;5;124m'\u001B[39m, when(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39misin(\u001B[38;5;28mlist\u001B[39m(states\u001B[38;5;241m.\u001B[39mkeys())),lit(states[col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m'\u001B[39m)]))\u001B[38;5;241m.\u001B[39motherwise(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n\n\u001B[0;31mNameError\u001B[0m: name 'lit' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'lit' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PySpark DataFrame Broadcast variable example\n",
    "from pyspark.sql.functions import col, when\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = ['firstname', 'lastname', 'country', 'state']\n",
    "df = spark.createDataFrame(data = data, schema=columns)\n",
    "df.printSchema()\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.withColumn('convertedstate', when(col('state').isin(list(states.keys())),lit(states[col('state')])).otherwise(col('state')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036c3dd8-cf51-43ee-b2a8-434db2c77554",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-503504833156610>:25\u001B[0m\n",
       "\u001B[1;32m     22\u001B[0m # Use when function to convert state codes to state names\n",
       "\u001B[1;32m     23\u001B[0m result = df.withColumn(\"Converted_state\", when(col(\"state\").isin(list(state_mapping.keys())), lit(state_mapping[col(\"state\")])).otherwise(\"Unknown\"))\n",
       "\u001B[0;32m---> 25\u001B[0m # Show the result\n",
       "\u001B[1;32m     26\u001B[0m result.show()\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: unhashable type: 'Column'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-503504833156610>:25\u001B[0m\n\u001B[1;32m     22\u001B[0m # Use when function to convert state codes to state names\n\u001B[1;32m     23\u001B[0m result = df.withColumn(\"Converted_state\", when(col(\"state\").isin(list(state_mapping.keys())), lit(state_mapping[col(\"state\")])).otherwise(\"Unknown\"))\n\u001B[0;32m---> 25\u001B[0m # Show the result\n\u001B[1;32m     26\u001B[0m result.show()\n\n\u001B[0;31mTypeError\u001B[0m: unhashable type: 'Column'",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: unhashable type: 'Column'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# Create a SparkSession (you would typically create a SparkSession)\n",
    "\n",
    "data = [(\"James\", \"Smith\", \"USA\", \"CA\"),\n",
    "        (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
    "        (\"Robert\", \"Williams\", \"USA\", \"CA\"),\n",
    "        (\"Maria\", \"Jones\", \"USA\", \"FL\")]\n",
    "\n",
    "columns = ['firstname', 'lastname', 'country', 'state']\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "\n",
    "# Define the state mapping using PySpark's when function\n",
    "states= {\n",
    "    \"NY\": \"New York\",\n",
    "    \"CA\": \"California\",\n",
    "    \"FL\": \"Florida\"\n",
    "}\n",
    "\n",
    "# Use when function to convert state codes to state names\n",
    "# result = df.withColumn(\"Converted_state\", when(col(\"state\").isin(list(state_mapping.keys())), lit(state_mapping[col(\"state\")])).otherwise(\"Unknown\"))\n",
    "\n",
    "result = df.withColumn('Converted_state', when(col('state').isin(list(states.keys())), lit(states[col('state')])).otherwise(col('state')))\n",
    "\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "269eedf0-c979-4c4e-b59e-b771239dc589",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n\nOut[2]: [('James', 'Smith', 'USA', 'California'),\n ('Michael', 'Rose', 'USA', 'New York'),\n ('Robert', 'Williams', 'USA', 'California'),\n ('Maria', 'Jones', 'USA', 'Florida')]"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession (you would typically create a SparkSession)\n",
    "\n",
    "data = [(\"James\", \"Smith\", \"USA\", \"CA\"),\n",
    "        (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
    "        (\"Robert\", \"Williams\", \"USA\", \"CA\"),\n",
    "        (\"Maria\", \"Jones\", \"USA\", \"FL\")]\n",
    "\n",
    "columns = ['firstname', 'lastname', 'country', 'state']\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema()\n",
    "\n",
    "# Define the state mapping using PySpark's when function\n",
    "states = {\"NY\": \"New York\", \"CA\": \"California\", \"FL\": \"Florida\"}\n",
    "\n",
    "# Use when function to convert state codes to state names\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],states[x[3]]))\n",
    "\n",
    "# Show the result\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1785687b-13f1-435e-a4f7-55b6460ded24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark Accumulator with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55814b5c-87a6-48f4-83b2-a50883d70460",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n65\n"
     ]
    }
   ],
   "source": [
    "#Creating Accumulator Variable\n",
    "\n",
    "accum = spark.sparkContext.accumulator(0)\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x: accum.add(x))\n",
    "print(accum.value)\n",
    "\n",
    "#with function\n",
    "accum1 = spark.sparkContext.accumulator(50)\n",
    "def sum_accum(x):\n",
    "    global accum1\n",
    "    accum1 += x\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(sum_accum)\n",
    "print(accum1.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71675ef9-ac67-4f75-afa7-0e94e53869eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Convert PySpark RDD to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d58780c-db26-40fc-a622-93c835e0169c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using rdd.toDF() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a5709e-86a8-42f1-a29e-e6d22a949df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n|       _1| _2|\n+---------+---+\n|  Finance| 10|\n|Marketing| 20|\n|    Sales| 30|\n|       IT| 40|\n+---------+---+\n\nroot\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "dept = [('Finance',10),('Marketing',20),('Sales',30),('IT', 40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "df=rdd.toDF()\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df2=rdd.toDF(['dept_name','dept_id'])\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b0b1bf3-3134-4359-987a-963ea8beece1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using PySpark createDataFrame() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e64216-0ba0-4655-93a7-679a2418a1c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(rdd, schema=['dept_name','dept_id'])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c609aae-b9a5-405b-b628-0db8253cd4c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using createDataFrame() with StructType schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ea8785-26a5-437b-ad69-89ca9d384a09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- dept_name: string (nullable = true)\n |-- dept_id: string (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "deptSchema = StructType([\n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema=deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70892bd5-d845-4242-945c-d16d7a6cac79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark StructType & StructField Explained with Examples\n",
    "\n",
    "**StructType** is a collection of **StructFields** that defines column name, column data type, bollena to specify if the field can be nullable or not and metadata\n",
    "* **StructType**: It Defines the Structure of DataFrame. It is a collection of list of StructField Objects.\n",
    "* **StructField** : It defines the metadata of the Dataframe column. It defines the columns which includes column name(string), column type(data type), nullable column(Boolean) and metadata(Metadata). Using StructField, we can also add nested struct schema, ArrayType for arrays , and MapType for key-value pairs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff8d891e-d381-4449-b44f-d58753a0bbb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "       StructField('firstname', StringType(), True), \\\n",
    "       StructField('middlename', StringType(), True), \\\n",
    "       StructField('lastname', StringType(), True), \\\n",
    "       StructField('id', StringType(), True), \\\n",
    "       StructField('gender', StringType(), True), \\\n",
    "       StructField('salary', StringType(), True)\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4a362c9-c936-46a8-9a29-db9cf03a42b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Defining Nested StructType object struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2881724-cfef-442a-bb2c-3859281e5ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3100  |\n|{Michael, Rose, }   |40288|M     |4300  |\n|{Robert, , Williams}|42114|M     |1400  |\n|{Maria, Anne, Jones}|39192|F     |5500  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "structureSchema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True), \n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "    ])),\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('salary', StringType(), True) \n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame(data=structureData, schema=structureSchema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "241aff86-5cfd-4428-b337-c31d8a29c73a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Using SQL ArrayType and MapType\n",
    "SQL StructType also supports ArrayType and MapType to define the DataFrame columns for array and map collections respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6800fd5-49c1-41a6-b7a2-74bf3d70fa28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType\n",
    "arrayStructureSchema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "    ])),\n",
    "    StructField('hobbies', ArrayType(StringType()), True),\n",
    "    StructField('properties', MapType(StringType(), StringType()),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9697d8b-7fe1-4a5f-b869-258844062fb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Convert PySpark DataFrame to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c795bce6-ca0d-4449-9ba5-b0c115f6d3bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)\n",
    "\n",
    "pandasDF = pysparkDF.toPandas()\n",
    "print(pandasDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50334986-4dae-4811-99ef-bc73544a1cb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Convert Spark Nested Struct DataFrame to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdaafc8-de88-42f6-9abf-45c3e7f332c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n\n                                                name    dob gender salary\n0  {'firstname': 'James', 'middlename': '', 'last...  36636      M   3000\n1  {'firstname': 'Michael', 'middlename': 'Rose',...  40288      M   4000\n2  {'firstname': 'Robert', 'middlename': '', 'las...  42114      M   4000\n3  {'firstname': 'Maria', 'middlename': 'Anne', '...  39192      F   4000\n4  {'firstname': 'Jen', 'middlename': 'Mary', 'la...             F     -1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Nested structure elements\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
    "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
    "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
    "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
    "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
    "]\n",
    "\n",
    "schemaStruct = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "          StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', StringType(), True)\n",
    "         ])\n",
    "df = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\n",
    "df.printSchema()\n",
    "\n",
    "pandasDF2 = df.toPandas()\n",
    "print(pandasDF2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be592b5-8e49-4e30-8863-ce9f69ea2a48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark show() – Display DataFrame Contents in Table\n",
    "\n",
    "Pyspark Dataframe show() is used to display the content of the Dataframe in a table row and column format. By efault it shows only 20 rows, and the column values are trubcated at 20 characters\n",
    "\n",
    "syntax :\n",
    "`\n",
    "def show(self, n=20, truncate=True, vertical=False):\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e134709-8927-43c4-85f6-90af0abe9897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n|                name|  dob|gender|salary|\n+--------------------+-----+------+------+\n|    {James, , Smith}|36636|     M|  3000|\n|   {Michael, Rose, }|40288|     M|  4000|\n|{Robert, , Williams}|42114|     M|  4000|\n|{Maria, Anne, Jones}|39192|     F|  4000|\n|  {Jen, Mary, Brown}|     |     F|    -1|\n+--------------------+-----+------+------+\n\n+--------------------+-----+------+------+\n|name                |dob  |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3000  |\n|{Michael, Rose, }   |40288|M     |4000  |\n|{Robert, , Williams}|42114|M     |4000  |\n|{Maria, Anne, Jones}|39192|F     |4000  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n+-----------------+-----+------+------+\n|name             |dob  |gender|salary|\n+-----------------+-----+------+------+\n|{James, , Smith} |36636|M     |3000  |\n|{Michael, Rose, }|40288|M     |4000  |\n+-----------------+-----+------+------+\nonly showing top 2 rows\n\n+-----------------+-----+------+------+\n|             name|  dob|gender|salary|\n+-----------------+-----+------+------+\n| {James, , Smith}|36636|     M|  3000|\n|{Michael, Rose, }|40288|     M|  4000|\n+-----------------+-----+------+------+\nonly showing top 2 rows\n\n-RECORD 0----------------------\n name   | {James, , Smith}     \n dob    | 36636                \n gender | M                    \n salary | 3000                 \n-RECORD 1----------------------\n name   | {Michael, Rose, }    \n dob    | 40288                \n gender | M                    \n salary | 4000                 \n-RECORD 2----------------------\n name   | {Robert, , Williams} \n dob    | 42114                \n gender | M                    \n salary | 4000                 \nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "#default - displays 2o rows and 20 characters from column value\n",
    "df.show()\n",
    "\n",
    "#display full column content\n",
    "df.show(truncate=False)\n",
    "\n",
    "#display 2 rows and full column contents\n",
    "df.show(2, truncate=False)\n",
    "\n",
    "#display 2 rows and column values 25 characters\n",
    "df.show(2, truncate=25)\n",
    "\n",
    "#display dataframe rows & columns vertically\n",
    "df.show(n=3, truncate=25, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68d9125-7434-476c-ba75-36aab1623044",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n|Seqno|               Quote|\n+-----+--------------------+\n|    1|Be the change tha...|\n|    2|Everyone thinks o...|\n|    3|The purpose of ou...|\n|    4|            Be cool.|\n+-----+--------------------+\n\n+-----+-----------------------------------------------------------------------------+\n|Seqno|Quote                                                                        |\n+-----+-----------------------------------------------------------------------------+\n|1    |Be the change that you wish to see in the world                              |\n|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n|3    |The purpose of our lives is to be happy.                                     |\n|4    |Be cool.                                                                     |\n+-----+-----------------------------------------------------------------------------+\n\n+-----+-----------------------------------------------------------------------------+\n|Seqno|Quote                                                                        |\n+-----+-----------------------------------------------------------------------------+\n|1    |Be the change that you wish to see in the world                              |\n|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n+-----+-----------------------------------------------------------------------------+\nonly showing top 2 rows\n\n-RECORD 0--------------------------\n Seqno | 1                         \n Quote | Be the change that you... \n-RECORD 1--------------------------\n Seqno | 2                         \n Quote | Everyone thinks of cha... \n-RECORD 2--------------------------\n Seqno | 3                         \n Quote | The purpose of our liv... \nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Quote\"]\n",
    "data = [(\"1\", \"Be the change that you wish to see in the world\"),\n",
    "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n",
    "    (\"3\", \"The purpose of our lives is to be happy.\"),\n",
    "    (\"4\", \"Be cool.\")]\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()\n",
    "df.show(truncate=False) #Display full column contents\n",
    "df.show(2, truncate=False) # Display 2 rows & column values 25 characters\n",
    "df.show(n=3,truncate=25,vertical=True) ## Display DataFrame rows & columns vertically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "723af3d0-0a7e-4186-b267-80b5832be2bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark Column Class | Operators & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddeb62f2-79f4-4db1-a64b-6cea0e9f51fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import lit\n",
    "colObj = lit(\"sparkbyexamples.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81bc4086-c447-4018-bd94-c63067f45f02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name.fname: string (nullable = true)\n |-- gender: long (nullable = true)\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [('James',23),('Ann', 40)]\n",
    "df = spark.createDataFrame(data).toDF('name.fname','gender')\n",
    "df.printSchema()\n",
    "\n",
    "df.select(df.gender).show()\n",
    "df.select(df[\"`name.fname`\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a434c00-9f3c-41ad-84e4-d982a67ef195",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name.fname: string (nullable = true)\n |-- gender: long (nullable = true)\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data=[(\"James\",23),(\"Ann\",40)]\n",
    "df=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
    "df.printSchema()\n",
    "df.select(df.gender).show()\n",
    "df.select(df[\"gender\"]).show()\n",
    "df.select(df[\"`name.fname`\"]).show()\n",
    "\n",
    "#Using SQL col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"gender\")).show()\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(col(\"`name.fname`\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce0dc764-2b38-4b49-a07b-f9a1218caa6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+---------+\n|prop.hair|\n+---------+\n|    black|\n|     grey|\n+---------+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black| blue|\n| grey|black|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create DataFrame with struct using Row class\n",
    "from pyspark.sql import Row\n",
    "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
    "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
    "df=spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.select(df.prop.hair).show()\n",
    "df.select(df[\"prop.hair\"]).show()\n",
    "df.select(col(\"prop.hair\")).show()\n",
    "df.select(col(\"prop.*\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67c92cef-7875-4385-9e03-4a0e64a5d0a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark Column Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94bf68d-a167-4329-90dc-f9d85eaf9b7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|(col1 + col2)|\n+-------------+\n|          102|\n|          203|\n|          304|\n+-------------+\n\n+-------------+\n|(col1 - col2)|\n+-------------+\n|           98|\n|          197|\n|          296|\n+-------------+\n\n+-------------+\n|(col1 * col2)|\n+-------------+\n|          200|\n|          600|\n|         1200|\n+-------------+\n\n+-----------------+\n|    (col1 / col2)|\n+-----------------+\n|             50.0|\n|66.66666666666667|\n|             75.0|\n+-----------------+\n\n+-------------+\n|(col1 % col2)|\n+-------------+\n|            0|\n|            2|\n|            0|\n+-------------+\n\n+-------------+\n|(col2 > col3)|\n+-------------+\n|         true|\n|        false|\n|        false|\n+-------------+\n\n+-------------+\n|(col2 < col3)|\n+-------------+\n|        false|\n|         true|\n|        false|\n+-------------+\n\n+-------------+\n|(col2 = col3)|\n+-------------+\n|        false|\n|        false|\n|         true|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
    "\n",
    "#Arthmetic operations\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show() \n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bff8d48b-52f7-4d45-88df-e4cf96c01e15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### alias() – Set’s name to Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d946374e-ebb8-420d-aaf6-bb32714d9bac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|first_name|last_name|\n+----------+---------+\n|     James|     Bond|\n|       Ann|    Varsa|\n|Tom Cruise|      XXX|\n| Tom Brand|     null|\n+----------+---------+\n\n+--------------+\n|      fullName|\n+--------------+\n|    James,Bond|\n|     Ann,Varsa|\n|Tom Cruise,XXX|\n|          null|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data=[(\"James\",\"Bond\",\"100\",None),\n",
    "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
    "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
    "      (\"Tom Brand\",None,\"400\",'M')] \n",
    "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "\n",
    "\n",
    "#alias\n",
    "from pyspark.sql.functions import expr\n",
    "df.select(df.fname.alias(\"first_name\"), \\\n",
    "          df.lname.alias(\"last_name\")\n",
    "   ).show()\n",
    "\n",
    "#Another example\n",
    "df.select(expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n",
    "   ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "094a2a0d-5065-4c0b-9b5b-8ae0ed7624c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### asc() & desc() – Sort the DataFrame columns by Ascending or Descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58dca0d4-d807-40f2-99c9-c5bc4a37d978",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|       Ann|Varsa|200|     F|\n|     James| Bond|100|  null|\n| Tom Brand| null|400|     M|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n|     James| Bond|100|  null|\n|       Ann|Varsa|200|     F|\n+----------+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#asc, desc to sort ascending and descending order repsectively.\n",
    "df.sort(df.fname.asc()).show()\n",
    "df.sort(df.fname.desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6019eda5-4cce-4ea4-a182-2d5d9a291855",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####cast() & astype() – Used to convert the data Type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99121d3a-7988-4b2b-8c8a-42c02c5b2fc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- fname: string (nullable = true)\n |-- id: string (nullable = true)\n\nroot\n |-- fname: string (nullable = true)\n |-- id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#cast\n",
    "df.select(df.fname,df.id).printSchema()\n",
    "df.select(df.fname,df.id.cast(\"int\")).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb1c6ec1-aa4e-4f62-822d-6b9173324303",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### between() – Returns a Boolean expression when a column values in between lower and upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f5c3e83-4ae6-44f7-83d0-a79fcbd2541e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+------+\n|fname|lname| id|gender|\n+-----+-----+---+------+\n|James| Bond|100|  null|\n|  Ann|Varsa|200|     F|\n+-----+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#between\n",
    "df.filter(df.id.between(100,300)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2810328-5820-4760-ad2a-1b902a95bb15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### contains() – Checks if a DataFrame column value contains a a value specified in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5288ccb-5637-41a8-9abf-b8ce8e67c144",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.fname.contains('Cruise')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb4f4bd4-98b7-445e-9273-0d19eef7e306",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### startswith() & endswith() – Checks if the value of the DataFrame Column starts and ends with a String respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d880cf-9a6c-4892-b694-952ffdc5c7bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.fname.startswith('T')).show()\n",
    "df.filter(df.fname.endswith('Cruise')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1feee52-4d96-47c0-bcde-c840dc795c68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "####isNull & isNotNull() – Checks if the DataFrame column has NULL or non NULL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bff6b1e8-1b2b-48c4-a2e8-61c6f549727b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---+------+\n|    fname|lname| id|gender|\n+---------+-----+---+------+\n|Tom Brand| null|400|     M|\n+---------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|     James| Bond|100|  null|\n|       Ann|Varsa|200|     F|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.lname.isNull()).show()\n",
    "df.filter(df.lname.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "784f89f0-1bf4-4b64-af14-5b926538b099",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### like() & rlike() – Similar to SQL LIKE expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc08ea89-3d21-4375-a865-181325f1a023",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: DataFrame[fname: string, lname: string, id: string]"
     ]
    }
   ],
   "source": [
    "\n",
    "#like , rlike\n",
    "df.select(df.fname,df.lname,df.id) \\\n",
    "  .filter(df.fname.like(\"%om\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a986051a-74e0-47f4-80f1-e243ae69e4ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### substr() – Returns a Column after getting sub string from the Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebcf191d-244a-4d17-a0c2-c899c0dbcd0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|SUBSTR|\n+------+\n|    Ja|\n|    An|\n|    To|\n|    To|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.fname.substr(1,2).alias('SUBSTR')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d214c6f4-5eb5-4698-8b5c-1d671e1e3cc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkByExample-3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
