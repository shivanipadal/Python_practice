{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5a35bee-e126-44af-ba3e-574ade065db1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## PySpark SQL Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45d396de-395f-43b9-9437-44f6505f6b7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###PySpark â€“ Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf624855-6d30-4e12-b0d7-e7a000e649a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Aggregate Function:\n",
    "\n",
    "* **approx_count_distinct()**: returns the count of distinct items in a group. It uses HyperLogLog (HLL) to estimate the count of distinct values with a relatively small margin of error. The trade-off is that it may not provide the exact count, but the error is controlled.\n",
    "* **avg()** : returns the average of the values in the input column\n",
    "* **collect_list()**: returns all values from an input column with Duplicates\n",
    "* **collect_set()**: returns all values from an input column with duplicate values eliminated.\n",
    "* **countDistinct()**: returns the number of distinct elements in a columns , This method is more accurate but can be slower, especially when dealing with large datasets or columns with high cardinality.\n",
    "* **count()**: returns number of elements in a column.\n",
    "* **first()**: returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element.\n",
    "* **last()**: returns the last element in a column when ignoreNulls is set to true, it returns the last non-null element.\n",
    "* **max()**: returns the maximum value in a column.\n",
    "* **min()**: returns the minimum value in a column.\n",
    "* **mean()**: returns the average of the values in a column. Alias for Avg\n",
    "* **stddev()**:  alias for stddev_samp.\n",
    "* **stddev_samp()**: function returns the sample standard deviation of values in a column.\n",
    "* **stddev_pop()**: function returns the population standard deviation of the values in a column.\n",
    "* **sum()**: function Returns the sum of all values in a column.\n",
    "* **sumDistinct()** : function returns the sum of all distinct values in a column.\n",
    "* **variance()**: alias for var_samp\n",
    "* **var_samp()**: function returns the unbiased variance of the values in a column.\n",
    "* **var_pop()**: function returns the population variance of the values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c2ae44-4e74-4b86-ae3d-2fa92c9cb916",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\napprox_count_distinct: 6\navg: 3400.0\n+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\nDistinct Count of Department & Salary: 8\ncount: Row(count(salary)=10)\n+-------------+\n|first(salary)|\n+-------------+\n|3000         |\n+-------------+\n\n+------------+\n|last(salary)|\n+------------+\n|4100        |\n+------------+\n\n+-------------------+\n|kurtosis(salary)   |\n+-------------------+\n|-0.6467803030303032|\n+-------------------+\n\n+-----------+\n|max(salary)|\n+-----------+\n|4600       |\n+-----------+\n\n+-----------+\n|min(salary)|\n+-----------+\n|2000       |\n+-----------+\n\n+-----------+\n|avg(salary)|\n+-----------+\n|3400.0     |\n+-----------+\n\n+--------------------+\n|skewness(salary)    |\n+--------------------+\n|-0.12041791181069571|\n+--------------------+\n\n+-------------------+-------------------+------------------+\n|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n+-------------------+-------------------+------------------+\n|765.9416862050705  |765.9416862050705  |726.636084983398  |\n+-------------------+-------------------+------------------+\n\n+-----------+\n|sum(salary)|\n+-----------+\n|34000      |\n+-----------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/functions.py:723: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|20900               |\n+--------------------+\n\n+-----------------+-----------------+---------------+\n|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n+-----------------+-----------------+---------------+\n|586666.6666666666|586666.6666666666|528000.0       |\n+-----------------+-----------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
    "\n",
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "  \n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "print(\"approx_count_distinct: \" + \\\n",
    "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
    "\n",
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
    "\n",
    "df.select(collect_list(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(collect_set(\"salary\")).show(truncate=False)\n",
    "\n",
    "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show(truncate=False)\n",
    "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0][0]))\n",
    "\n",
    "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
    "df.select(first(\"salary\")).show(truncate=False)\n",
    "df.select(last(\"salary\")).show(truncate=False)\n",
    "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
    "df.select(max(\"salary\")).show(truncate=False)\n",
    "df.select(min(\"salary\")).show(truncate=False)\n",
    "df.select(mean(\"salary\")).show(truncate=False)\n",
    "df.select(skewness(\"salary\")).show(truncate=False)\n",
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
    "    stddev_pop(\"salary\")).show(truncate=False)\n",
    "df.select(sum(\"salary\")).show(truncate=False)\n",
    "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
    "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
    "  .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3f104c9-38db-40a1-a177-69eaca5883bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Pyspark Windows function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f11f5f3b-b328-46fd-b8ed-327cb70e9852",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d79f3b0-d867-4f38-b5f5-ab4817b6a092",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####row_number Window Function\n",
    "\n",
    "row_number(): window function is used to give the sequential row number starting from 1 to the result of each window partition.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e203985d-a8e9-4c24-8108-ca5a8328c8d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|Maria        |Finance   |3000  |1         |\n|Scott        |Finance   |3300  |2         |\n|Jen          |Finance   |3900  |3         |\n|Kumar        |Marketing |2000  |1         |\n|Jeff         |Marketing |3000  |2         |\n|James        |Sales     |3000  |1         |\n|James        |Sales     |3000  |2         |\n|Robert       |Sales     |4100  |3         |\n|Saif         |Sales     |4100  |4         |\n|Michael      |Sales     |4600  |5         |\n+-------------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy('department').orderBy('Salary')\n",
    "\n",
    "df.withColumn('row_number', row_number().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce70372b-75d9-4c32-a1f4-896308b70a6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### rank Window Function\n",
    "rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318c0466-ee62-4153-a35b-25bdd01cd487",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+--------+\n|employee_name|department|salary|rank_num|\n+-------------+----------+------+--------+\n|Maria        |Finance   |3000  |1       |\n|Scott        |Finance   |3300  |2       |\n|Jen          |Finance   |3900  |3       |\n|Kumar        |Marketing |2000  |1       |\n|Jeff         |Marketing |3000  |2       |\n|James        |Sales     |3000  |1       |\n|James        |Sales     |3000  |1       |\n|Robert       |Sales     |4100  |3       |\n|Saif         |Sales     |4100  |3       |\n|Michael      |Sales     |4600  |5       |\n+-------------+----------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "windowSpec=Window.partitionBy('department').orderBy('salary')\n",
    "\n",
    "df.withColumn(\"rank_num\", rank().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1051ede-bc98-40df-9776-3cd12fdf8623",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### dense_rank Window Function\n",
    "window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee42fb6-d026-49cd-ab87-4956197c8f6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|Maria        |Finance   |3000  |1         |\n|Scott        |Finance   |3300  |2         |\n|Jen          |Finance   |3900  |3         |\n|Kumar        |Marketing |2000  |1         |\n|Jeff         |Marketing |3000  |2         |\n|James        |Sales     |3000  |1         |\n|James        |Sales     |3000  |1         |\n|Robert       |Sales     |4100  |2         |\n|Saif         |Sales     |4100  |2         |\n|Michael      |Sales     |4600  |3         |\n+-------------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn('dense_rank', dense_rank().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66db7ffd-ad27-48fd-916d-26e77dcd5008",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####percent_rank Window Function\n",
    " It assigns a value between 0 and 1 to each row, indicating its position within the ordered set of rows. The PERCENT_RANK function assigns a value between 0 and 1 to each row, where 0 represents the first row (lowest value) in the sorted partition, and 1 represents the last row (highest value) in the partition. Rows with the same values in the ORDER BY columns will receive the same PERCENT_RANK value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b48277-3f79-4d40-9d38-8e5cb31340fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n|employee_name|department|salary|percent_rank|\n+-------------+----------+------+------------+\n|Maria        |Finance   |3000  |0.0         |\n|Scott        |Finance   |3300  |0.5         |\n|Jen          |Finance   |3900  |1.0         |\n|Kumar        |Marketing |2000  |0.0         |\n|Jeff         |Marketing |3000  |1.0         |\n|James        |Sales     |3000  |0.0         |\n|James        |Sales     |3000  |0.0         |\n|Robert       |Sales     |4100  |0.5         |\n|Saif         |Sales     |4100  |0.5         |\n|Michael      |Sales     |4600  |1.0         |\n+-------------+----------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "df.withColumn('percent_rank', percent_rank().over(windowsSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38fc3f7f-94b1-4d6d-a190-6922d61f30c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####ntile Window Function:\n",
    "window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c89914-7fdd-4558-bf60-abfb6c6c4103",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n|employee_name|department|salary|ntile|\n+-------------+----------+------+-----+\n|        Maria|   Finance|  3000|    1|\n|        Scott|   Finance|  3300|    1|\n|          Jen|   Finance|  3900|    2|\n|        Kumar| Marketing|  2000|    1|\n|         Jeff| Marketing|  3000|    2|\n|        James|     Sales|  3000|    1|\n|        James|     Sales|  3000|    1|\n|       Robert|     Sales|  4100|    1|\n|         Saif|     Sales|  4100|    2|\n|      Michael|     Sales|  4600|    2|\n+-------------+----------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2041b958-3e85-494a-a43c-fcd8f75eaaa7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### PySpark Window Analytic functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6eb1e57-150f-4d56-be26-e92e1a495730",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### cume_dist Window Function\n",
    "cume_dist() window function is used to get the cumulative distribution of values within a window partition.\n",
    "\n",
    "This is the same as the DENSE_RANK function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1434ffe-1c93-4c41-99e3-25fe617832e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n|employee_name|department|salary|         cume_dist|\n+-------------+----------+------+------------------+\n|        Maria|   Finance|  3000|0.3333333333333333|\n|        Scott|   Finance|  3300|0.6666666666666666|\n|          Jen|   Finance|  3900|               1.0|\n|        Kumar| Marketing|  2000|               0.5|\n|         Jeff| Marketing|  3000|               1.0|\n|        James|     Sales|  3000|               0.4|\n|        James|     Sales|  3000|               0.4|\n|       Robert|     Sales|  4100|               0.8|\n|         Saif|     Sales|  4100|               0.8|\n|      Michael|     Sales|  4600|               1.0|\n+-------------+----------+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" cume_dist \"\"\"\n",
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
    "   .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "317ddfde-e19e-4ffb-839a-85e1cb8f19fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### lag Window Function\n",
    "The LAG function is a window function that allows you to access the value of a previous row within the result set. It's often used to perform calculations that involve comparing the current row with a previous row in a specified order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed819116-c137-490e-954a-034f01700668",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary| lag|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|null|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|3000|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|null|\n|        James|     Sales|  3000|null|\n|       Robert|     Sales|  4100|3000|\n|         Saif|     Sales|  4100|3000|\n|      Michael|     Sales|  4600|4100|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "df.withColumn('lag', lag('salary', 2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79a09832-e8af-4fac-a743-d79bbf830c57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### lead in sql \n",
    "the 'lead' function is a window function that allows you to access the value of a subsequent row within the result set. It often used to perform calculations that involves comparing current row with a following row in a specific order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d721af9b-964e-419f-b40c-2bb2d9c6d557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary|lead|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|3900|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|null|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|4100|\n|        James|     Sales|  3000|4100|\n|       Robert|     Sales|  4100|4600|\n|         Saif|     Sales|  4100|null|\n|      Michael|     Sales|  4600|null|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "df.withColumn('lead', lead('salary',2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23ab0c00-69d1-4347-80e8-9dd83ce5f57d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####PySpark Window Aggregate Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da031f8-a854-40bb-b1c6-bf8f0e4e7272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+------+-----+----+----+\n|employee_name|department|salary|row|avg   |sum  |min |max |\n+-------------+----------+------+---+------+-----+----+----+\n|Maria        |Finance   |3000  |1  |3400.0|10200|3000|3900|\n|Scott        |Finance   |3300  |2  |3400.0|10200|3000|3900|\n|Jen          |Finance   |3900  |3  |3400.0|10200|3000|3900|\n|Kumar        |Marketing |2000  |1  |2500.0|5000 |2000|3000|\n|Jeff         |Marketing |3000  |2  |2500.0|5000 |2000|3000|\n|James        |Sales     |3000  |1  |3760.0|18800|3000|4600|\n|James        |Sales     |3000  |2  |3760.0|18800|3000|4600|\n|Robert       |Sales     |4100  |3  |3760.0|18800|3000|4600|\n|Saif         |Sales     |4100  |4  |3760.0|18800|3000|4600|\n|Michael      |Sales     |4600  |5  |3760.0|18800|3000|4600|\n+-------------+----------+------+---+------+-----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg = Window.partitionBy('department')\n",
    "from pyspark.sql.functions import col, avg, sum, min, max, row_number\n",
    "\n",
    "df.withColumn('row', row_number().over(windowSpec)) \\\n",
    "  .withColumn('avg', avg(col('salary')).over(windowSpecAgg)) \\\n",
    "  .withColumn('sum', sum(col('salary')).over(windowSpecAgg)) \\\n",
    "  .withColumn('min', min(col('salary')).over(windowSpecAgg)) \\\n",
    "  .withColumn('max', max(col('salary')).over(windowSpecAgg)) \\\n",
    "  .show(truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4955797e-5d5c-4068-8e20-225a33bd9065",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark SQL Date and Timestamp Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f928f349-1310-4c04-996c-36653189dfcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb0803d-5914-407d-a165-3436fc6ac5c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n| id|     input|\n+---+----------+\n|  1|2020-02-01|\n|  2|2019-03-01|\n|  3|2021-03-01|\n+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7260dc58-dec6-42e5-bcfd-fa3e4a02d5a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### current_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59be1ead-67c3-4f96-bc8d-4ea040972234",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+\n|id |input     |Current_date|\n+---+----------+------------+\n|1  |2020-02-01|2023-09-11  |\n|2  |2019-03-01|2023-09-11  |\n|3  |2021-03-01|2023-09-11  |\n+---+----------+------------+\n\n+------------+\n|current_date|\n+------------+\n|2023-09-11  |\n|2023-09-11  |\n|2023-09-11  |\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Current_date', current_date()).show(truncate=False)\n",
    "\n",
    "df.select(current_date().alias('current_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d3e3162-17ed-4154-bf41-87ffa3c4f4d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### date_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca3e089b-441f-488b-89ae-9a277d29eab8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+\n| id|     input|Date_format|\n+---+----------+-----------+\n|  1|2020-02-01| 02-01-2020|\n|  2|2019-03-01| 03-01-2019|\n|  3|2021-03-01| 03-01-2021|\n+---+----------+-----------+\n\n+----------+-----------+\n|     input|Date_format|\n+----------+-----------+\n|2020-02-01| 02-01-2020|\n|2019-03-01| 03-01-2019|\n|2021-03-01| 03-01-2021|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Date_format', date_format(df.input,'MM-dd-yyyy')).show()\n",
    "\n",
    "df.select(col('input'), date_format(col('input'), \"MM-dd-yyyy\").alias('Date_format')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99cf4296-2eda-4d5e-bd05-030501fdea86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####to_date():\n",
    "Below example converts string in date format yyyy-MM-dd to a DateType yyyy-MM-dd using to_date(). You can also use this to convert into any specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "914e9f18-27f7-4390-9d9f-5bf487ef5732",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|     input|   to_date|\n+----------+----------+\n|2020-02-01|2020-02-01|\n|2019-03-01|2019-03-01|\n|2021-03-01|2021-03-01|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col('input'), to_date(col('input'), 'yyyy-MM-dd').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff1c7afd-5b93-4e7b-ad05-2ed7dc1b20d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### datediff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59feb0e5-b9b9-4bcf-91bb-9ed937a0ba4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|input     |date_diff|\n+----------+---------+\n|2020-02-01|1318     |\n|2019-03-01|1655     |\n|2021-03-01|924      |\n+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col('input'), datediff(current_date(), col('input')).alias('date_diff')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8308de1-43b4-4407-90c0-9183e8428772",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####months_between()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf29157-f9af-4ee3-99b4-d1927f58e8ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n|input     |months_between|\n+----------+--------------+\n|2020-02-01|43.32258065   |\n|2019-03-01|54.32258065   |\n|2021-03-01|30.32258065   |\n+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col('input'), months_between(current_date(), col('input')).alias('months_between')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79bc74cf-47a8-434f-94bb-71648dea9038",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####trunc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecf4a57-3030-40b7-a69f-4a5d38ed5783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----------+\n|     input|Month_Trunc|Month_Year|Month_Trunc|\n+----------+-----------+----------+-----------+\n|2020-02-01| 2020-02-01|2020-01-01| 2020-02-01|\n|2019-03-01| 2019-03-01|2019-01-01| 2019-03-01|\n|2021-03-01| 2021-03-01|2021-01-01| 2021-03-01|\n+----------+-----------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"), \n",
    "    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86d58fd2-0282-43e8-8aff-3b0b2b3b5f41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####add_months() , date_add(), date_sub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f216ae-bd9c-40db-afaa-8b18c629c10a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n|     input|add_months|sub_months|  date_add|  date_sub|\n+----------+----------+----------+----------+----------+\n|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n+----------+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), \n",
    "    add_months(col(\"input\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"input\"),4).alias(\"date_add\"), \n",
    "    date_sub(col(\"input\"),4).alias(\"date_sub\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d15c096-2742-4b31-87d3-9cc91c4caa17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####year(), month(),next_day(), weekofyear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe42345-e040-4546-b5c3-d46e290e6a48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+----------+----------+\n|input     |year|month|next_day  |weekofyear|\n+----------+----+-----+----------+----------+\n|2020-02-01|2020|2    |2020-02-02|5         |\n|2019-03-01|2019|3    |2019-03-03|9         |\n|2021-03-01|2021|3    |2021-03-07|9         |\n+----------+----+-----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col('input'),\n",
    "          year(col('input')).alias('year'),\n",
    "          month(col('input')).alias('month'),\n",
    "          next_day(col('input'),'Sunday').alias('next_day'),\n",
    "          weekofyear(col('input')).alias('weekofyear')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "068a308c-1321-4584-a53b-6232ecc6546f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### dayofweek(), dayofmonth(), dayofyear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7642ab02-1ab8-4958-9b4c-e2b08f328003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n|     input|dayofweek|dayofmonth|dayofyear|\n+----------+---------+----------+---------+\n|2020-02-01|        7|         1|       32|\n|2019-03-01|        6|         1|       60|\n|2021-03-01|        2|         1|       60|\n+----------+---------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"),  \n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c10e2cb5-747b-428d-bd1e-19f251c9b17e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### current_timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c737c78e-ef82-4da6-a894-8274422d4398",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n|current_timestamp      |\n+-----------------------+\n|2023-09-11 11:21:32.619|\n+-----------------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp().alias('current_timestamp')).show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91117c31-a9ab-4ca2-8279-48d9f0295fb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####to_timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d1b806-181b-43ee-bfad-4052e68c10cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n|id |input                  |\n+---+-----------------------+\n|1  |02-01-2020 11 01 19 06 |\n|2  |03-01-2019 12 01 19 406|\n|3  |03-01-2021 12 01 19 406|\n+---+-----------------------+\n\n+-----------------------+-----------------------+\n|input                  |To-Timestamp           |\n+-----------------------+-----------------------+\n|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n+-----------------------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df2.select(col('input'), \n",
    "           to_timestamp(col('input'), \"MM-dd-yyyy HH mm ss SSS\").alias('To-Timestamp')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b21fe14-e515-474e-ace4-de3f0a5cfc33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####hour(), Minute() and second()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8014693-d1ee-46fb-b0de-f16fdb723fc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n|input                  |Hour|minute|second|\n+-----------------------+----+------+------+\n|2020-02-01 11:01:19.06 |11  |1     |19    |\n|2019-03-01 12:01:19.406|12  |1     |19    |\n|2021-03-01 12:01:19.406|12  |1     |19    |\n+-----------------------+----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "df3.select(col('input'),\n",
    "           hour(col('input')).alias('Hour'),\n",
    "           minute(col('input')).alias('minute'),\n",
    "           second(col('input')).alias('second')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "280caff0-6742-47a9-a9a2-df4294e7e522",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------------------+\n|id |value                                                                          |\n+---+-------------------------------------------------------------------------------+\n|1  |{\"zipcode\":704, \"ZipCodeType\":\"STANDARD\", \"City\": \"PARC PARQUE\", \"State\": \"PR\"}|\n+---+-------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "jsonString = \"\"\"{\"zipcode\":704, \"ZipCodeType\":\"STANDARD\", \"City\": \"PARC PARQUE\", \"State\": \"PR\"}\"\"\"\n",
    "df=spark.createDataFrame([(1, jsonString)],['id','value'])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17400f57-0b8a-4895-a109-9aae47e60718",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23978e2-0620-499c-aa45-58a94337c25f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- value: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+---+---------------------------------------------------------------------------+\n|id |value                                                                      |\n+---+---------------------------------------------------------------------------+\n|1  |{zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n+---+---------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Convert JSON string column to Map type\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "df2=df.withColumn('value', from_json(df.value, MapType(StringType(),StringType())))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95343cf7-4068-4c43-9abd-39237bdf03fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####to_json\n",
    "convert DataFrame columns MapType or Struct type to JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b16af033-8ce4-45ab-962b-277a4e5a853a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------+\n|id |value                                                                       |\n+---+----------------------------------------------------------------------------+\n|1  |{\"zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n+---+----------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json, col\n",
    "df2.withColumn('value',to_json(col('value'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43cb357e-e484-4662-967b-b954be55b23c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####json_tuple()\n",
    "\n",
    "it is used the query or extract the elements from json column and create result as a new column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3817f85-be36-45da-902b-0c48f996dddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+-----+\n|id |ZipCodeType|City       |State|\n+---+-----------+-----------+-----+\n|1  |STANDARD   |PARC PARQUE|PR   |\n+---+-----------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "df.select(col('id'), json_tuple(col('value'),'ZipCodeType','City', 'State')) \\\n",
    "    .toDF('id', 'ZipCodeType', 'City','State') \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1abb36f-fd78-4e22-af51-2e3416e9f92d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### get_json_object()\n",
    "it is used to extract the json string based on path from the json column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b595555-a985-4b73-9a8d-9df3f2412bdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n|id |ZipCodeType|\n+---+-----------+\n|1  |STANDARD   |\n+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "df.select(col('id'), get_json_object(col('value'), \"$.ZipCodeType\").alias('ZipCodeType')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abd84bcb-4512-4388-9572-139b24b47117",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####schema_of_json()\n",
    "It is used to create a schema string from json string column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f29c9dae-863d-4f6c-ba40-e55dd4badf70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRUCT<City: STRING, State: STRING, ZipCodeType: STRING, Zipcode: BIGINT>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import schema_of_json,lit\n",
    "schemaStr=spark.range(1) \\\n",
    "          .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n",
    "           .collect()[0][0]\n",
    "\n",
    "print(schemaStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fc13530-80d7-4a9d-bd84-c901df4fdcc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkByExample -7 (PySpark SQL Functions)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
