{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7517d4ee-9ea0-4714-9f6a-9353d61ee47e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.master('local[*]').appName('SparkByExample').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "229a8461-71c5-42d8-832d-6187425f0b98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark UDF (User Defined Function)\n",
    "\n",
    "\n",
    "* **Why do we need a UDF** : UDF's are user to extend the functions of the framework and re-use these functions on multiple dataframes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d0d004-82b9-412b-90b3-da3d3fa56c7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8d800d-62ee-473c-aec1-6eac1849f2c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "def convertCase(x):\n",
    "    return x[0:1].upper() + x[1:len(x)] \n",
    "\n",
    "convertcaseudf = udf(lambda z: convertCase(z), StringType()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb231e03-ff61-4157-b7a5-afb54bc783ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n|seqno|Name        |\n+-----+------------+\n|1    |John jones  |\n|2    |Tracey smith|\n|3    |Amy sanders |\n+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select(col('seqno'), \\\n",
    "    convertcaseudf(col('Name')).alias('Name')) \\\n",
    "    .show(truncate=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fdfab67-247e-4cf9-a964-92a61cac9cd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n|Seqno|        Name|Modified name|\n+-----+------------+-------------+\n|    1|  john jones|   JOHN JONES|\n|    2|tracey smith| TRACEY SMITH|\n|    3| amy sanders|  AMY SANDERS|\n+-----+------------+-------------+\n\nNone\n"
     ]
    }
   ],
   "source": [
    "def upperCase(x):\n",
    "    return x.upper()\n",
    "\n",
    "uppercaseUDF = udf(lambda z: upperCase(z), StringType())\n",
    "\n",
    "df1 = df.withColumn(\"Modified name\", uppercaseUDF(df.Name))\n",
    "print(df1.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1102075-ca23-427f-9637-eccf8a0b0d20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Creating UDF using annotation:\n",
    "\n",
    "Using annotation, we can avoid two steps . That is i) Create a python function, ii) convert function to udf function using SQL udf() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f28bb2c-6650-4035-a473-e48ae4abb35d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n|Seqno|        Name|  Upper Name|\n+-----+------------+------------+\n|    1|  john jones|  JOHN JONES|\n|    2|tracey smith|TRACEY SMITH|\n|    3| amy sanders| AMY SANDERS|\n+-----+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "@udf(returnType=StringType())\n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "df2=df.withColumn(\"Upper Name\", upperCase(df.Name))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b22d4874-3d4e-4496-bcfe-978dd2993d7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n|Seqno|        name|\n+-----+------------+\n|    1|  John jones|\n|    2|Tracey smith|\n|    3| Amy sanders|\n+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Registering PySpark UDF & use it on SQL\n",
    "\n",
    "##Using idf on sql###\n",
    "spark.udf.register(\"convertCase\", convertCase,StringType())\n",
    "df.createOrReplaceTempView(\"Name_Table\")\n",
    "spark.sql(\"select Seqno, convertCase(Name) as name from Name_Table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0c708cb-aa53-46ed-87fb-fb249a68e75a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark transform() Function with Example\n",
    "\n",
    "syntax :\n",
    "```\n",
    "\n",
    "# Syntax\n",
    "DataFrame.transform(func: Callable[[…], DataFrame], *args: Any, **kwargs: Any) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f35d3f-e7ad-400b-958a-b6917adbf206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- CourseName: string (nullable = true)\n |-- fee: long (nullable = true)\n |-- discount: long (nullable = true)\n\n+----------+----+--------+\n|CourseName|fee |discount|\n+----------+----+--------+\n|Java      |4000|5       |\n|Python    |4600|10      |\n|Scala     |4100|15      |\n|Scala     |4500|15      |\n|PHP       |3000|20      |\n+----------+----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('SparkByExamples.com') \\\n",
    "            .getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "simpleData = ((\"Java\",4000,5), \\\n",
    "    (\"Python\", 4600,10),  \\\n",
    "    (\"Scala\", 4100,15),   \\\n",
    "    (\"Scala\", 4500,15),   \\\n",
    "    (\"PHP\", 3000,20),  \\\n",
    "  )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e86d52-919f-489b-9812-94d1e0ec73ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-------+--------------+\n|coursename| fee|discount|new_fee|discounted_fee|\n+----------+----+--------+-------+--------------+\n|      JAVA|4000|       5|   3000|        2850.0|\n|    PYTHON|4600|      10|   3600|        3240.0|\n|     SCALA|4100|      15|   3100|        2635.0|\n|     SCALA|4500|      15|   3500|        2975.0|\n|       PHP|3000|      20|   2000|        1600.0|\n+----------+----+--------+-------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#custom tranformation 1\n",
    "from pyspark.sql.functions import upper \n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn('coursename', upper(df.CourseName))\n",
    "\n",
    "#custom transformation 2\n",
    "def reduce_price(df,reduceBy):\n",
    "    return df.withColumn('new_fee', df.fee - reduceBy)\n",
    "\n",
    "#custom transformation 3\n",
    "def apply_discount(df):\n",
    "    return df.withColumn('discounted_fee', df.new_fee - (df.new_fee * df.discount) / 100)\n",
    "\n",
    "#Apply dataframe transform\n",
    "\n",
    "df2 = df.transform(to_upper_str_columns) \\\n",
    "    .transform(reduce_price, 1000) \\\n",
    "    .transform(apply_discount)\n",
    "\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c61578-d811-404e-9282-fcf109de96d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### map() and flatmap()\n",
    "\n",
    "* **map()**:\n",
    "  - The map() transformation applies given function to each element of an RDD and return new RDD with the results\n",
    "  - It produces a **one-to-one mapping**, meaning that for each input element, teh function generates excatly one output element\n",
    "* **flatmap()**:\n",
    "  - the flatmao() transformation applies a given function to each element of an RDD an returns a new RDD by flattening the result\n",
    "  - It produces **one-to many mapping**, meaning that for each input element, the function can generate zero, one or multiple output elements.\n",
    "  - it doesnot support psypark dataframe. it suuports only for RDD. For pyspark dataframe , we can use `explode` functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e033e9f2-8e74-4c04-99a2-10d11f2f0a12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: ['1', '2', '2', '1', '1', '3', '4', '1']"
     ]
    }
   ],
   "source": [
    "#map()\n",
    "\n",
    "# rdd=sc.parallelize([12,21,31,41,51])\n",
    "# sq_rdd=rdd.map(lambda x: x**2)\n",
    "# sq_rdd.collect()\n",
    "\n",
    "rdd1=sc.parallelize(['1-2','2-1','1-3','4-1'])\n",
    "rdd_flat_map =rdd1.flatMap(lambda x: x.split('-'))\n",
    "rdd_flat_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f226b6fd-1b33-4c04-8181-69f20cb152ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### foreach() \n",
    "\n",
    "- It is used to apply a function to each element of an RDD/DF without returning new RDD/DF.i.e, it is **action**\n",
    "- syntax: `rdd.foreach(func)`\n",
    "- the foreach() action in PySpark does not return any values, and it does not necessarily print to the console. Instead, it is typically used for side effects like updating external resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b2e655-a345-4bfc-8d6a-20b818d6a138",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "\n",
    "def print_element(ele):\n",
    "    print(ele)\n",
    "\n",
    "rdd.foreach(print_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f2ea32-9982-4265-b3a3-e8473a3f5250",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkByExample - 5",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
