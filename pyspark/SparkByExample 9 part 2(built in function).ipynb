{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad8a8b4e-6a2f-4fcd-8697-5c540908346d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### PySpark – create_map()\n",
    "\n",
    "#####PySpark Convert DataFrame Columns to MapType (Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd578401-02c1-49e9-98c6-bbb6ed2e5b83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkbyexample.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0fad39-df25-41d1-954f-76d39244b6e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [ (\"36636\",\"Finance\",3000,\"USA\"), \n",
    "    (\"40288\",\"Finance\",5000,\"IND\"), \n",
    "    (\"42114\",\"Sales\",3900,\"USA\"), \n",
    "    (\"39192\",\"Marketing\",2500,\"CAN\"), \n",
    "    (\"34534\",\"Sales\",6500,\"USA\") ]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('dept', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True),\n",
    "    StructField('location', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2089a6-b6f3-416b-9ba2-06dce528f2bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------+--------+\n|   id|     dept|salary|location|\n+-----+---------+------+--------+\n|36636|  Finance|  3000|     USA|\n|40288|  Finance|  5000|     IND|\n|42114|    Sales|  3900|     USA|\n|39192|Marketing|  2500|     CAN|\n|34534|    Sales|  6500|     USA|\n+-----+---------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8d4e19-3fad-4899-bcc3-69a06a3e01de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- propetiesMap: map (nullable = false)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+-----+---------+---------------------------------+\n|id   |dept     |propetiesMap                     |\n+-----+---------+---------------------------------+\n|36636|Finance  |{salary -> 3000, location -> USA}|\n|40288|Finance  |{salary -> 5000, location -> IND}|\n|42114|Sales    |{salary -> 3900, location -> USA}|\n|39192|Marketing|{salary -> 2500, location -> CAN}|\n|34534|Sales    |{salary -> 6500, location -> USA}|\n+-----+---------+---------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, create_map\n",
    "df = df.withColumn('propetiesMap', create_map(\n",
    "    lit('salary'), col('salary'),\n",
    "    lit('location'), col('location')\n",
    ")).drop('salary', 'location')\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca3c9a63-7668-4e4c-ac15-0f1a3cdc2cb5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####  map_keys() – Get All Map Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99452c2-9bd5-4450-bbce-244227e48531",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------------------+\n|   id|     dept|map_keys(propetiesMap)|\n+-----+---------+----------------------+\n|36636|  Finance|    [salary, location]|\n|40288|  Finance|    [salary, location]|\n|42114|    Sales|    [salary, location]|\n|39192|Marketing|    [salary, location]|\n|34534|    Sales|    [salary, location]|\n+-----+---------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "df.select('id', 'dept', map_keys('propetiesMap')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdccddc2-1173-4aa0-873f-65f3e8c8c5e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####map_values() – Get All map Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad1cb94-5860-4877-889b-d12603e0ecf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------------------+\n|   id|     dept|map_values(propetiesMap)|\n+-----+---------+------------------------+\n|36636|  Finance|             [3000, USA]|\n|40288|  Finance|             [5000, IND]|\n|42114|    Sales|             [3900, USA]|\n|39192|Marketing|             [2500, CAN]|\n|34534|    Sales|             [6500, USA]|\n+-----+---------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "df.select('id', 'dept', map_values('propetiesMap')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab585c5b-d34b-4913-905c-3010758bb0ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### PySpark – struct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e815da3a-c781-4687-8cd0-5faca6849533",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3100  |\n|{Michael, Rose, }   |40288|M     |4300  |\n|{Robert, , Williams}|42114|M     |1400  |\n|{Maria, Anne, Jones}|39192|F     |5500  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3ecfa6-534e-480b-a645-634d0268d46e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- Otherinfo: struct (nullable = false)\n |    |-- identifier: string (nullable = true)\n |    |-- Gender: string (nullable = true)\n |    |-- salary: integer (nullable = true)\n |    |-- Salary_Grade: string (nullable = false)\n\n+--------------------+------------------------+\n|name                |Otherinfo               |\n+--------------------+------------------------+\n|{James, , Smith}    |{36636, M, 3100, Medium}|\n|{Michael, Rose, }   |{40288, M, 4300, High}  |\n|{Robert, , Williams}|{42114, M, 1400, low}   |\n|{Maria, Anne, Jones}|{39192, F, 5500, High}  |\n|{Jen, Mary, Brown}  |{, F, -1, low}          |\n+--------------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, when\n",
    "df_new = df2.withColumn('Otherinfo', struct(col('id').alias('identifier'), \n",
    "                                   col('gender').alias('Gender'),\n",
    "                                   col('Salary').alias('salary'),\n",
    "                                   when(col('salary').cast(IntegerType()) < 2000, 'low')\n",
    "                                   .when(col('salary').cast(IntegerType()) < 4000, 'Medium')\n",
    "                                   .otherwise('High').alias('Salary_Grade')\n",
    "                                   )).drop('id','gender','salary')\n",
    "\n",
    "df_new.printSchema()\n",
    "df_new.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6689b91-2c5f-4856-b6d4-493a33b9baf3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### countDistinct() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8639c6-cd9d-4f85-974a-686ba7876858",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n|count(DISTINCT dept, salary)|\n+----------------------------+\n|                           8|\n+----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "columns = [\"Name\",\"Dept\",\"Salary\"]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "# df.show()\n",
    "\n",
    "df.count()\n",
    "df.distinct().count()  #returns a new DataFrame after eliminating duplicate rows (distinct on all columns)\n",
    "\n",
    "# if you want to get count distinct on selected multiple columns, use the PySpark SQL function countDistinct(). This function returns the number of distinct elements in a group.\n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct('dept', 'salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aab7775a-d5b2-4289-b045-ec92fc8fd12d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### PySpark – sum(), avg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8269cbfc-cf2c-4d46-83fc-60f01a9a3734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NV   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |DE   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |NV   |80000 |25 |18000|\n|Kumar        |Marketing |NJ   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "282df6a8-c33c-4f33-86b9-8cd0804828f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|department|sum(Salary)|\n+----------+-----------+\n|     Sales|     257000|\n|   Finance|     351000|\n| Marketing|     171000|\n+----------+-----------+\n\n+----------+-----------------+\n|department|      avg(Salary)|\n+----------+-----------------+\n|     Sales|85666.66666666667|\n|   Finance|          87750.0|\n| Marketing|          85500.0|\n+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').sum('Salary').show()\n",
    "df.groupBy('department').avg('Salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c70da8ec-8395-45fe-b197-5fb1c950821c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####row_number Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d5ea323-a0d5-433d-b947-2a79e844a46c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n|department|salary|\n+----------+------+\n|   Finance| 83000|\n| Marketing| 91000|\n|     Sales| 86000|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy('department').orderBy('Salary')\n",
    "\n",
    "df = df.withColumn('New_row', row_number().over(windowSpec))\n",
    "df.select('department','salary').filter(\"New_row==2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "625d5e23-6937-47f3-95e6-20108825d216",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### rank Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5097119-c11a-45c9-90a5-f6335f7dd40a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "134c9d62-b4da-4205-9f57-a9b5abff940f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|   1|\n|        Scott|   Finance|  3300|   2|\n|          Jen|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|         Jeff| Marketing|  3000|   2|\n|        James|     Sales|  3000|   1|\n|        James|     Sales|  3000|   1|\n|       Robert|     Sales|  4100|   3|\n|         Saif|     Sales|  4100|   3|\n|      Michael|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "windowSpec = Window.partitionBy('department').orderBy('salary')\n",
    "df_new = df.withColumn('rank', rank().over(windowSpec))\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c3af967-dd15-4db4-ad7f-61356d6786bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3790592e-301c-47f2-a4af-17e2ea832ddc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n+-------------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"dens_rank\"\"\"\n",
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de529e51-d00b-4fc6-ac3f-c92809350f62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkByExample 9 (built in function)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
